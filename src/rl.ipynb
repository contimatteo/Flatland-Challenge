{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from flatland.utils.rendertools import AgentRenderVariant, RenderTool\n",
    "from rl.core import Env\n",
    "from experiments.dqn_test.training_environment import FlatTreeObs, generate_environment\n",
    "import numpy as np\n",
    "\n",
    "class FlatlandEnv(Env):\n",
    "    def __init__(self, n_agents=1, seed=1000, max_steps=200):\n",
    "        self._seed = seed\n",
    "        self._obs_builder = FlatTreeObs(1)\n",
    "        self._env, self._renderer = generate_environment(n_agents, self._obs_builder, seed)\n",
    "        self._steps = 0\n",
    "        self._max_steps = max_steps\n",
    "        self._nb_resets = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # supports only single agent\n",
    "        # print(action)\n",
    "\n",
    "        action = 2 if action == 4 else action\n",
    "        obs, reward, done, info = self._env.step({0: action})\n",
    "        score = reward[0]\n",
    "        while not info['action_required'][0] and not done[0]:\n",
    "            obs, reward, done, info = self._env.step({0: 3})\n",
    "            score += reward[0]\n",
    "        # obs = tf.constant([obs[0]], dtype=tf.float32)\n",
    "        # print(obs)\n",
    "        obs = obs[0]\n",
    "        if type(obs) == list:\n",
    "            obs = np.array(obs)\n",
    "        else:\n",
    "            obs = np.zeros(self._obs_builder.state_size())\n",
    "        #print(obs, reward[0], done[0])\n",
    "\n",
    "        if self._steps >= self._max_steps:\n",
    "            done[0] = True,\n",
    "            score -= 100\n",
    "        self._steps += 1\n",
    "        return obs, score, done[0], {}\n",
    "\n",
    "    def reset(self):\n",
    "        try:\n",
    "            self._renderer.close_window()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        self._steps = 0\n",
    "        self._nb_resets += 1\n",
    "        if self._nb_resets > 2:\n",
    "            random.seed(self._seed)\n",
    "            self._seed = random.randint(0, 100000)\n",
    "            self._nb_resets = 0\n",
    "\n",
    "        obs, _ = self._env.reset(random_seed=self._seed, activate_agents=True)\n",
    "        self._renderer = RenderTool(\n",
    "            self._env,\n",
    "            show_debug=True,\n",
    "            agent_render_variant=AgentRenderVariant.AGENT_SHOWS_OPTIONS_AND_BOX,\n",
    "            screen_height=500,\n",
    "            screen_width=500)\n",
    "\n",
    "        obs, _, _, _ = self.step(2)\n",
    "        #self._renderer.reset()\n",
    "        obs = obs[0]\n",
    "        obs = np.array(obs)\n",
    "        if obs.size < self._obs_builder.state_size():\n",
    "            obs = np.zeros(self._obs_builder.state_size())\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        self._renderer.render_env(show=True)\n",
    "        time.sleep(0.001)\n",
    "\n",
    "    def close(self):\n",
    "        self._renderer.close_window()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self._seed = seed\n",
    "        return [seed]\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def action_space(self):\n",
    "        return self._env.action_space[0]  # returns 5\n",
    "\n",
    "    def states_space(self):\n",
    "        return self._obs_builder.state_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "env = FlatlandEnv()\n",
    "states = env.states_space()\n",
    "actions = env.action_space()\n",
    "print(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "episodes = 0# CHANGE Here\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        #print(n_state)\n",
    "        score += reward\n",
    "        #time.sleep(0.1)\n",
    "    time.sleep(1)\n",
    "    print(f\"Episode: {episode}, Score: {score}\")\n",
    "if episodes > 0:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input, InputLayer\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    _model = Sequential()\n",
    "    _model.add(Flatten(input_shape=(1,states)))\n",
    "    _model.add(Dense(20, activation='relu'))\n",
    "    _model.add(Dense(10, activation='relu'))\n",
    "    _model.add(Dense(actions, activation='linear'))\n",
    "\n",
    "    # input = Input(shape=(1,states))\n",
    "    # x = Flatten(input)\n",
    "    # x = Dense(24, activation='relu')(x)\n",
    "    # x = Dense(24, activation='relu')(x)\n",
    "    # output = Dense(5, activation='linear')(x)\n",
    "    # model = Model(inputs=)\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_8 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 485\n",
      "Trainable params: 485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent          # Deep q learning agent\n",
    "from rl.policy import BoltzmannQPolicy  # Policy based reinforcement learning\n",
    "from rl.memory import SequentialMemory  # Memory ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=5000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions,\n",
    "                   nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "   149/50000: episode: 1, duration: 5.304s, episode steps: 149, steps per second: 28, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.235 [0.000, 4.000], mean observation: 197.667 [0.000, 1000.000], loss: 1054.936241, mae: 55.433181, mean_q: 52.904121\n",
      "   298/50000: episode: 2, duration: 3.168s, episode steps: 149, steps per second: 47, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.369 [0.000, 4.000], mean observation: 136.119 [0.000, 1000.000], loss: 106.385139, mae: 54.704735, mean_q: 51.252617\n",
      "   447/50000: episode: 3, duration: 3.193s, episode steps: 149, steps per second: 47, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.866 [0.000, 4.000], mean observation: 146.736 [0.000, 1000.000], loss: 24.885792, mae: 49.546009, mean_q: 44.238796\n",
      "   596/50000: episode: 4, duration: 3.089s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.698 [0.000, 4.000], mean observation: 146.736 [0.000, 1000.000], loss: 9.829268, mae: 45.041115, mean_q: 39.357182\n",
      "   745/50000: episode: 5, duration: 3.177s, episode steps: 149, steps per second: 47, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.007 [0.000, 4.000], mean observation: 146.736 [0.000, 1000.000], loss: 8.631398, mae: 42.464066, mean_q: 36.639648\n",
      "   894/50000: episode: 6, duration: 3.071s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: 202.600 [0.000, 1000.000], loss: 6.310265, mae: 41.445541, mean_q: 35.387581\n",
      "  1043/50000: episode: 7, duration: 3.014s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.013 [0.000, 2.000], mean observation: 202.600 [0.000, 1000.000], loss: 8.650083, mae: 40.503117, mean_q: 35.871021\n",
      "  1192/50000: episode: 8, duration: 3.018s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.027 [0.000, 4.000], mean observation: 202.600 [0.000, 1000.000], loss: 34.740974, mae: 40.586231, mean_q: 46.021061\n",
      "  1341/50000: episode: 9, duration: 2.958s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.383 [0.000, 4.000], mean observation: 205.900 [0.000, 1000.000], loss: 147.290878, mae: 47.293587, mean_q: 59.647953\n",
      "  1490/50000: episode: 10, duration: 2.960s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 205.900 [0.000, 1000.000], loss: 206.687881, mae: 51.408195, mean_q: 62.454113\n",
      "  1639/50000: episode: 11, duration: 2.949s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.651 [0.000, 4.000], mean observation: 205.900 [0.000, 1000.000], loss: 131.598190, mae: 52.041817, mean_q: 61.468651\n",
      "  1653/50000: episode: 12, duration: 2.022s, episode steps: 14, steps per second: 7, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 3.286 [0.000, 4.000], mean observation: 108.729 [0.000, 1000.000], loss: 144.851486, mae: 51.963734, mean_q: 59.569000\n",
      "  1734/50000: episode: 13, duration: 2.531s, episode steps: 81, steps per second: 32, episode reward: -79.000, mean reward: -0.975 [-1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 155.778 [0.000, 1000.000], loss: 121.521606, mae: 53.625603, mean_q: 60.806919\n",
      "  1806/50000: episode: 14, duration: 2.451s, episode steps: 72, steps per second: 29, episode reward: -70.000, mean reward: -0.972 [-1.000, 1.000], mean action: 2.403 [0.000, 4.000], mean observation: 120.747 [0.000, 1000.000], loss: 83.902802, mae: 52.605415, mean_q: 58.933418\n",
      "  1955/50000: episode: 15, duration: 3.003s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.148 [0.000, 4.000], mean observation: 131.907 [0.000, 1000.000], loss: 67.420708, mae: 53.989410, mean_q: 59.657845\n",
      "  2104/50000: episode: 16, duration: 2.976s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.396 [0.000, 4.000], mean observation: 130.331 [0.000, 1000.000], loss: 88.535179, mae: 62.320465, mean_q: 72.680962\n",
      "  2253/50000: episode: 17, duration: 2.997s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.711 [0.000, 4.000], mean observation: 129.787 [0.000, 1000.000], loss: 71.365402, mae: 73.360878, mean_q: 88.341942\n",
      "  2256/50000: episode: 18, duration: 1.893s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.200 [0.000, 2.000], loss: 31.078669, mae: 77.981758, mean_q: 92.838104\n",
      "  2259/50000: episode: 19, duration: 1.905s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 2.000], loss: 30.491693, mae: 78.873169, mean_q: 94.297363\n",
      "  2262/50000: episode: 20, duration: 1.907s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.200 [0.000, 2.000], loss: 27.670099, mae: 79.547523, mean_q: 94.823723\n",
      "  2411/50000: episode: 21, duration: 3.102s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.966 [0.000, 4.000], mean observation: 121.945 [0.000, 1000.000], loss: 56.823910, mae: 79.733681, mean_q: 93.720879\n",
      "  2560/50000: episode: 22, duration: 3.041s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.013 [0.000, 4.000], mean observation: 127.977 [0.000, 1000.000], loss: 56.596233, mae: 78.346748, mean_q: 90.659599\n",
      "  2709/50000: episode: 23, duration: 3.058s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.866 [0.000, 4.000], mean observation: 142.122 [0.000, 1000.000], loss: 51.359787, mae: 76.001465, mean_q: 87.687538\n",
      "  2858/50000: episode: 24, duration: 2.992s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.315 [0.000, 4.000], mean observation: 135.311 [0.000, 1000.000], loss: 39.037006, mae: 72.564857, mean_q: 83.987343\n",
      "  3007/50000: episode: 25, duration: 3.013s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.141 [0.000, 4.000], mean observation: 134.597 [0.000, 1000.000], loss: 36.080254, mae: 70.257683, mean_q: 80.982819\n",
      "  3156/50000: episode: 26, duration: 2.984s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.503 [0.000, 4.000], mean observation: 137.199 [0.000, 1000.000], loss: 34.412636, mae: 68.014206, mean_q: 78.061188\n",
      "  3305/50000: episode: 27, duration: 3.014s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 3.805 [0.000, 4.000], mean observation: 108.579 [0.000, 1000.000], loss: 37.657688, mae: 65.220261, mean_q: 74.523323\n",
      "  3454/50000: episode: 28, duration: 3.010s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 3.195 [0.000, 4.000], mean observation: 132.946 [0.000, 1000.000], loss: 33.846069, mae: 61.799118, mean_q: 70.565407\n",
      "  3603/50000: episode: 29, duration: 3.019s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.591 [0.000, 4.000], mean observation: 147.157 [0.000, 1000.000], loss: 31.016386, mae: 58.977062, mean_q: 67.071281\n",
      "  3604/50000: episode: 30, duration: 1.858s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 61.521347, mae: 58.077324, mean_q: 64.716423\n",
      "  3605/50000: episode: 31, duration: 1.873s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.000 [0.000, 0.000], loss: 60.680271, mae: 58.142677, mean_q: 64.418449\n",
      "  3606/50000: episode: 32, duration: 1.850s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 59.944447, mae: 58.374794, mean_q: 63.954674\n",
      "  3755/50000: episode: 33, duration: 2.968s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.114 [0.000, 4.000], mean observation: 128.483 [0.000, 1000.000], loss: 24.592279, mae: 56.334759, mean_q: 63.830780\n",
      "  3829/50000: episode: 34, duration: 2.402s, episode steps: 74, steps per second: 31, episode reward: -72.000, mean reward: -0.973 [-1.000, 1.000], mean action: 2.541 [0.000, 4.000], mean observation: 142.384 [0.000, 1000.000], loss: 19.585176, mae: 54.938843, mean_q: 62.145409\n",
      "  3978/50000: episode: 35, duration: 2.977s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.154 [0.000, 4.000], mean observation: 158.423 [0.000, 1000.000], loss: 20.561750, mae: 53.382092, mean_q: 59.809689\n",
      "  4127/50000: episode: 36, duration: 3.076s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.872 [0.000, 4.000], mean observation: 143.230 [0.000, 1000.000], loss: 20.844189, mae: 51.567986, mean_q: 57.037491\n",
      "  4276/50000: episode: 37, duration: 2.998s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.221 [0.000, 4.000], mean observation: 144.583 [0.000, 1000.000], loss: 15.902396, mae: 49.478733, mean_q: 54.403542\n",
      "  4425/50000: episode: 38, duration: 2.979s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.047 [0.000, 4.000], mean observation: 144.346 [0.000, 1000.000], loss: 13.903124, mae: 47.595703, mean_q: 51.822262\n",
      "  4431/50000: episode: 39, duration: 1.788s, episode steps: 6, steps per second: 3, episode reward: -4.000, mean reward: -0.667 [-1.000, 1.000], mean action: 2.500 [1.000, 4.000], mean observation: 50.617 [0.000, 1000.000], loss: 21.455164, mae: 47.520828, mean_q: 50.035522\n",
      "  4484/50000: episode: 40, duration: 2.098s, episode steps: 53, steps per second: 25, episode reward: -51.000, mean reward: -0.962 [-1.000, 1.000], mean action: 2.491 [0.000, 4.000], mean observation: 124.372 [0.000, 1000.000], loss: 17.153305, mae: 45.943298, mean_q: 49.924942\n",
      "  4602/50000: episode: 41, duration: 2.620s, episode steps: 118, steps per second: 45, episode reward: -116.000, mean reward: -0.983 [-1.000, 1.000], mean action: 1.822 [0.000, 4.000], mean observation: 153.897 [0.000, 1000.000], loss: 13.035624, mae: 45.086735, mean_q: 48.641659\n",
      "  4751/50000: episode: 42, duration: 3.115s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 3.805 [0.000, 4.000], mean observation: 103.521 [0.000, 1000.000], loss: 10.416152, mae: 43.308308, mean_q: 46.116695\n",
      "  4900/50000: episode: 43, duration: 2.928s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.953 [0.000, 4.000], mean observation: 103.521 [0.000, 1000.000], loss: 9.272899, mae: 41.095905, mean_q: 43.113224\n",
      "  5049/50000: episode: 44, duration: 2.989s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.443 [0.000, 4.000], mean observation: 104.878 [0.000, 1000.000], loss: 7.190140, mae: 38.790165, mean_q: 40.074631\n",
      "  5198/50000: episode: 45, duration: 2.866s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.745 [0.000, 4.000], mean observation: 166.803 [0.000, 1000.000], loss: 6.913753, mae: 36.511520, mean_q: 36.973675\n",
      "  5347/50000: episode: 46, duration: 2.858s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.101 [0.000, 4.000], mean observation: 168.838 [0.000, 1000.000], loss: 7.079727, mae: 35.001831, mean_q: 34.780884\n",
      "  5496/50000: episode: 47, duration: 2.858s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.557 [0.000, 4.000], mean observation: 159.998 [0.000, 1000.000], loss: 5.397974, mae: 33.604198, mean_q: 32.695255\n",
      "  5522/50000: episode: 48, duration: 1.873s, episode steps: 26, steps per second: 14, episode reward: -24.000, mean reward: -0.923 [-1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 148.304 [0.000, 1000.000], loss: 3.887674, mae: 32.268009, mean_q: 31.391071\n",
      "  5671/50000: episode: 49, duration: 2.787s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 152.811 [0.000, 1000.000], loss: 235.500137, mae: 33.168198, mean_q: 33.235313\n",
      "  5685/50000: episode: 50, duration: 1.812s, episode steps: 14, steps per second: 8, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 3.143 [0.000, 4.000], mean observation: 101.764 [0.000, 1000.000], loss: 5.934722, mae: 32.036045, mean_q: 30.781816\n",
      "  5834/50000: episode: 51, duration: 3.073s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.477 [0.000, 4.000], mean observation: 130.734 [0.000, 1000.000], loss: 144.643143, mae: 31.737730, mean_q: 30.837425\n",
      "  5983/50000: episode: 52, duration: 2.887s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.074 [0.000, 4.000], mean observation: 129.809 [0.000, 1000.000], loss: 132.296188, mae: 32.264732, mean_q: 31.574806\n",
      "  6132/50000: episode: 53, duration: 2.789s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.342 [0.000, 4.000], mean observation: 129.384 [0.000, 1000.000], loss: 6.380149, mae: 29.830872, mean_q: 28.012548\n",
      "  6281/50000: episode: 54, duration: 2.767s, episode steps: 149, steps per second: 54, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.114 [0.000, 4.000], mean observation: 143.294 [0.000, 1000.000], loss: 40.876736, mae: 28.790045, mean_q: 26.612104\n",
      "  6430/50000: episode: 55, duration: 2.869s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.228 [0.000, 4.000], mean observation: 201.275 [0.000, 1000.000], loss: 5.660769, mae: 26.677910, mean_q: 24.354397\n",
      "  6579/50000: episode: 56, duration: 2.785s, episode steps: 149, steps per second: 54, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.275 [0.000, 4.000], mean observation: 168.950 [0.000, 1000.000], loss: 4.679111, mae: 25.430618, mean_q: 22.834742\n",
      "  6728/50000: episode: 57, duration: 2.775s, episode steps: 149, steps per second: 54, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 4.000], mean observation: 151.095 [0.000, 1000.000], loss: 4.247051, mae: 24.158291, mean_q: 21.496801\n",
      "  6877/50000: episode: 58, duration: 2.780s, episode steps: 149, steps per second: 54, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.758 [0.000, 4.000], mean observation: 132.433 [0.000, 1000.000], loss: 4.279151, mae: 23.017342, mean_q: 20.151106\n",
      "  7026/50000: episode: 59, duration: 2.844s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 4.000], mean observation: 137.713 [0.000, 1000.000], loss: 4.682026, mae: 21.901783, mean_q: 18.973072\n",
      "  7175/50000: episode: 60, duration: 2.927s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.027 [0.000, 4.000], mean observation: 153.481 [0.000, 1000.000], loss: 3.318359, mae: 20.733404, mean_q: 17.940935\n",
      "  7324/50000: episode: 61, duration: 2.934s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.013 [0.000, 4.000], mean observation: 136.009 [0.000, 1000.000], loss: 3.017547, mae: 19.855829, mean_q: 16.224789\n",
      "  7473/50000: episode: 62, duration: 2.928s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.832 [0.000, 4.000], mean observation: 134.685 [0.000, 1000.000], loss: 2.636781, mae: 18.592983, mean_q: 14.857653\n",
      "  7622/50000: episode: 63, duration: 2.907s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.899 [0.000, 4.000], mean observation: 104.382 [0.000, 1000.000], loss: 2.263226, mae: 17.735544, mean_q: 12.690692\n",
      "  7667/50000: episode: 64, duration: 2.184s, episode steps: 45, steps per second: 21, episode reward: -43.000, mean reward: -0.956 [-1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 99.384 [0.000, 1000.000], loss: 2.179985, mae: 17.070833, mean_q: 11.275502\n",
      "  7741/50000: episode: 65, duration: 2.386s, episode steps: 74, steps per second: 31, episode reward: -72.000, mean reward: -0.973 [-1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 100.292 [0.000, 1000.000], loss: 1.851668, mae: 16.738800, mean_q: 10.249672\n",
      "  7742/50000: episode: 66, duration: 1.982s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 3.203580, mae: 16.952770, mean_q: 7.071239\n",
      "  7743/50000: episode: 67, duration: 1.891s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 1.526366, mae: 17.269539, mean_q: 7.286745\n",
      "  7744/50000: episode: 68, duration: 1.906s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 1.051623, mae: 15.708858, mean_q: 7.917546\n",
      "  7893/50000: episode: 69, duration: 2.922s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.148 [0.000, 4.000], mean observation: 130.219 [0.000, 1000.000], loss: 2.259299, mae: 15.043051, mean_q: 9.697961\n",
      "  8042/50000: episode: 70, duration: 2.918s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.081 [0.000, 4.000], mean observation: 130.248 [0.000, 1000.000], loss: 1.796608, mae: 12.845187, mean_q: 8.944552\n",
      "  8191/50000: episode: 71, duration: 2.946s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.148 [0.000, 4.000], mean observation: 132.962 [0.000, 1000.000], loss: 1.473553, mae: 11.802526, mean_q: 8.102302\n",
      "  8194/50000: episode: 72, duration: 1.828s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 2.000], loss: 1.611926, mae: 10.250361, mean_q: 8.803922\n",
      "  8197/50000: episode: 73, duration: 1.823s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 3.333 [3.000, 4.000], mean observation: 0.200 [0.000, 2.000], loss: 0.143306, mae: 11.215001, mean_q: 8.468604\n",
      "  8200/50000: episode: 74, duration: 1.875s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 2.000], loss: 3.414902, mae: 9.719905, mean_q: 8.672712\n",
      "  8349/50000: episode: 75, duration: 2.917s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.443 [0.000, 4.000], mean observation: 165.592 [0.000, 1000.000], loss: 1.114140, mae: 9.842788, mean_q: 7.584518\n",
      "  8498/50000: episode: 76, duration: 3.080s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.181 [0.000, 4.000], mean observation: 159.603 [0.000, 1000.000], loss: 0.908917, mae: 8.618342, mean_q: 6.689673\n",
      "  8606/50000: episode: 77, duration: 2.622s, episode steps: 108, steps per second: 41, episode reward: -106.000, mean reward: -0.981 [-1.000, 1.000], mean action: 1.972 [0.000, 4.000], mean observation: 155.989 [0.000, 1000.000], loss: 0.963138, mae: 7.613122, mean_q: 5.758318\n",
      "  8755/50000: episode: 78, duration: 3.133s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.671 [0.000, 4.000], mean observation: 123.392 [0.000, 1000.000], loss: 0.683432, mae: 6.602399, mean_q: 4.725751\n",
      "  8904/50000: episode: 79, duration: 3.003s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.933 [0.000, 4.000], mean observation: 123.392 [0.000, 1000.000], loss: 0.530681, mae: 5.236450, mean_q: 3.434473\n",
      "  9053/50000: episode: 80, duration: 2.978s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.906 [0.000, 4.000], mean observation: 129.383 [0.000, 1000.000], loss: 0.657863, mae: 4.167290, mean_q: 2.059166\n",
      "  9202/50000: episode: 81, duration: 3.001s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.181 [0.000, 4.000], mean observation: 126.326 [0.000, 1000.000], loss: 0.627258, mae: 3.582773, mean_q: 1.289159\n",
      "  9351/50000: episode: 82, duration: 3.135s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.087 [0.000, 4.000], mean observation: 135.876 [0.000, 1000.000], loss: 0.515384, mae: 3.369761, mean_q: 1.091499\n",
      "  9500/50000: episode: 83, duration: 2.991s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.040 [0.000, 4.000], mean observation: 134.621 [0.000, 1000.000], loss: 0.407387, mae: 3.159866, mean_q: 0.876102\n",
      "  9548/50000: episode: 84, duration: 2.154s, episode steps: 48, steps per second: 22, episode reward: -46.000, mean reward: -0.958 [-1.000, 1.000], mean action: 2.354 [0.000, 4.000], mean observation: 148.090 [0.000, 1000.000], loss: 0.424914, mae: 3.310376, mean_q: 0.760067\n",
      "  9618/50000: episode: 85, duration: 2.307s, episode steps: 70, steps per second: 30, episode reward: -68.000, mean reward: -0.971 [-1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 134.821 [0.000, 1000.000], loss: 0.403687, mae: 3.218394, mean_q: 0.647087\n",
      "  9644/50000: episode: 86, duration: 2.055s, episode steps: 26, steps per second: 13, episode reward: -24.000, mean reward: -0.923 [-1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 128.896 [0.000, 1000.000], loss: 0.395457, mae: 3.669653, mean_q: 0.541408\n",
      "  9691/50000: episode: 87, duration: 2.166s, episode steps: 47, steps per second: 22, episode reward: -45.000, mean reward: -0.957 [-1.000, 1.000], mean action: 2.106 [0.000, 4.000], mean observation: 125.038 [0.000, 1000.000], loss: 0.306156, mae: 3.169992, mean_q: 0.493391\n",
      "  9817/50000: episode: 88, duration: 2.775s, episode steps: 126, steps per second: 45, episode reward: -124.000, mean reward: -0.984 [-1.000, 1.000], mean action: 2.365 [0.000, 4.000], mean observation: 128.145 [0.000, 1000.000], loss: 0.322860, mae: 3.182563, mean_q: 0.330321\n",
      "  9854/50000: episode: 89, duration: 2.180s, episode steps: 37, steps per second: 17, episode reward: -35.000, mean reward: -0.946 [-1.000, 1.000], mean action: 2.243 [0.000, 4.000], mean observation: 128.549 [0.000, 1000.000], loss: 0.325591, mae: 2.892198, mean_q: 0.217428\n",
      " 10003/50000: episode: 90, duration: 2.973s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.282 [0.000, 4.000], mean observation: 128.948 [0.000, 1000.000], loss: 0.339842, mae: 2.964813, mean_q: 0.095584\n",
      " 10152/50000: episode: 91, duration: 2.908s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.060 [0.000, 4.000], mean observation: 138.213 [0.000, 1000.000], loss: 0.427340, mae: 3.098672, mean_q: -0.030170\n",
      " 10301/50000: episode: 92, duration: 2.958s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.107 [0.000, 4.000], mean observation: 138.505 [0.000, 1000.000], loss: 0.273042, mae: 3.525833, mean_q: -0.199766\n",
      " 10444/50000: episode: 93, duration: 2.698s, episode steps: 143, steps per second: 53, episode reward: -141.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 134.387 [0.000, 1000.000], loss: 0.290331, mae: 3.511673, mean_q: -0.325885\n",
      " 10498/50000: episode: 94, duration: 2.078s, episode steps: 54, steps per second: 26, episode reward: -52.000, mean reward: -0.963 [-1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 118.906 [0.000, 1000.000], loss: 0.294487, mae: 3.842076, mean_q: -0.402267\n",
      " 10610/50000: episode: 95, duration: 2.471s, episode steps: 112, steps per second: 45, episode reward: -110.000, mean reward: -0.982 [-1.000, 1.000], mean action: 2.161 [0.000, 4.000], mean observation: 117.486 [0.000, 1000.000], loss: 0.300688, mae: 3.586969, mean_q: -0.475310\n",
      " 10759/50000: episode: 96, duration: 5.871s, episode steps: 149, steps per second: 25, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.530 [0.000, 4.000], mean observation: 144.170 [0.000, 1000.000], loss: 0.296276, mae: 3.861058, mean_q: -0.621123\n",
      " 10863/50000: episode: 97, duration: 4.880s, episode steps: 104, steps per second: 21, episode reward: -102.000, mean reward: -0.981 [-1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 138.925 [0.000, 1000.000], loss: 0.289731, mae: 3.802003, mean_q: -0.713129\n",
      " 11012/50000: episode: 98, duration: 4.787s, episode steps: 149, steps per second: 31, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.369 [0.000, 4.000], mean observation: 143.550 [0.000, 1000.000], loss: 0.269244, mae: 3.837667, mean_q: -0.803262\n",
      " 11161/50000: episode: 99, duration: 4.498s, episode steps: 149, steps per second: 33, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.322 [0.000, 4.000], mean observation: 138.261 [0.000, 1000.000], loss: 0.261660, mae: 4.192981, mean_q: -0.901111\n",
      " 11251/50000: episode: 100, duration: 4.184s, episode steps: 90, steps per second: 22, episode reward: -88.000, mean reward: -0.978 [-1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 135.403 [0.000, 1000.000], loss: 1.400167, mae: 4.179278, mean_q: -0.729073\n",
      " 11400/50000: episode: 101, duration: 3.815s, episode steps: 149, steps per second: 39, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.329 [0.000, 4.000], mean observation: 127.737 [0.000, 1000.000], loss: 0.737486, mae: 4.010328, mean_q: -0.930917\n",
      " 11549/50000: episode: 102, duration: 3.771s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.309 [0.000, 4.000], mean observation: 137.130 [0.000, 1000.000], loss: 0.240066, mae: 4.006646, mean_q: -1.129291\n",
      " 11698/50000: episode: 103, duration: 3.642s, episode steps: 149, steps per second: 41, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.383 [0.000, 4.000], mean observation: 139.951 [0.000, 1000.000], loss: 0.244041, mae: 4.056219, mean_q: -1.262891\n",
      " 11847/50000: episode: 104, duration: 3.692s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.483 [0.000, 4.000], mean observation: 138.608 [0.000, 1000.000], loss: 0.249424, mae: 4.527889, mean_q: -1.362269\n",
      " 11996/50000: episode: 105, duration: 3.543s, episode steps: 149, steps per second: 42, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.403 [0.000, 4.000], mean observation: 117.273 [0.000, 1000.000], loss: 0.250792, mae: 4.939359, mean_q: -1.468436\n",
      " 12145/50000: episode: 106, duration: 3.435s, episode steps: 149, steps per second: 43, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.362 [0.000, 4.000], mean observation: 131.530 [0.000, 1000.000], loss: 0.263437, mae: 5.286994, mean_q: -1.593164\n",
      " 12294/50000: episode: 107, duration: 3.378s, episode steps: 149, steps per second: 44, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.456 [0.000, 4.000], mean observation: 133.977 [0.000, 1000.000], loss: 0.298069, mae: 5.291638, mean_q: -1.748409\n",
      " 12392/50000: episode: 108, duration: 3.076s, episode steps: 98, steps per second: 32, episode reward: -96.000, mean reward: -0.980 [-1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 110.157 [0.000, 1000.000], loss: 0.380832, mae: 5.461287, mean_q: -1.803913\n",
      " 12417/50000: episode: 109, duration: 2.494s, episode steps: 25, steps per second: 10, episode reward: -23.000, mean reward: -0.920 [-1.000, 1.000], mean action: 1.840 [0.000, 4.000], mean observation: 110.360 [0.000, 1000.000], loss: 0.895874, mae: 5.368085, mean_q: -1.849008\n",
      " 12428/50000: episode: 110, duration: 2.350s, episode steps: 11, steps per second: 5, episode reward: -9.000, mean reward: -0.818 [-1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 110.227 [0.000, 1000.000], loss: 0.373557, mae: 5.578166, mean_q: -1.902053\n",
      " 12577/50000: episode: 111, duration: 3.876s, episode steps: 149, steps per second: 38, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.141 [0.000, 4.000], mean observation: 147.290 [0.000, 1000.000], loss: 0.295281, mae: 5.301719, mean_q: -1.997174\n",
      " 12726/50000: episode: 112, duration: 3.649s, episode steps: 149, steps per second: 41, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 140.682 [0.000, 1000.000], loss: 0.309348, mae: 5.237758, mean_q: -2.150020\n",
      " 12875/50000: episode: 113, duration: 3.494s, episode steps: 149, steps per second: 43, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.289 [0.000, 4.000], mean observation: 139.704 [0.000, 1000.000], loss: 0.340379, mae: 5.453946, mean_q: -2.286395\n",
      " 12883/50000: episode: 114, duration: 2.348s, episode steps: 8, steps per second: 3, episode reward: -6.000, mean reward: -0.750 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 88.100 [0.000, 1000.000], loss: 0.299073, mae: 5.597483, mean_q: -2.381920\n",
      " 13016/50000: episode: 115, duration: 3.447s, episode steps: 133, steps per second: 39, episode reward: -131.000, mean reward: -0.985 [-1.000, 1.000], mean action: 2.391 [0.000, 4.000], mean observation: 131.123 [0.000, 1000.000], loss: 0.322989, mae: 5.506110, mean_q: -2.437701\n",
      " 13165/50000: episode: 116, duration: 3.792s, episode steps: 149, steps per second: 39, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.201 [0.000, 4.000], mean observation: 128.738 [0.000, 1000.000], loss: 0.319957, mae: 5.530879, mean_q: -2.589676\n",
      " 13314/50000: episode: 117, duration: 3.384s, episode steps: 149, steps per second: 44, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.248 [0.000, 4.000], mean observation: 132.468 [0.000, 1000.000], loss: 0.326585, mae: 5.676163, mean_q: -2.712671\n",
      " 13463/50000: episode: 118, duration: 3.382s, episode steps: 149, steps per second: 44, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.141 [0.000, 4.000], mean observation: 130.400 [0.000, 1000.000], loss: 0.344927, mae: 5.684862, mean_q: -2.848762\n",
      " 13612/50000: episode: 119, duration: 3.443s, episode steps: 149, steps per second: 43, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.503 [0.000, 4.000], mean observation: 129.657 [0.000, 1000.000], loss: 0.343373, mae: 6.056221, mean_q: -3.002373\n",
      " 13614/50000: episode: 120, duration: 2.239s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.100 [0.000, 1.000], loss: 0.435487, mae: 5.818073, mean_q: -2.989699\n",
      " 13616/50000: episode: 121, duration: 2.205s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 2.500 [1.000, 4.000], mean observation: 0.100 [0.000, 1.000], loss: 0.445736, mae: 4.788139, mean_q: -3.099127\n",
      " 13618/50000: episode: 122, duration: 2.255s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 0.100 [0.000, 1.000], loss: 0.341685, mae: 7.762205, mean_q: -3.103954\n",
      "re_generate cnt=2\n",
      " 13767/50000: episode: 123, duration: 3.719s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.443 [0.000, 4.000], mean observation: 127.919 [0.000, 1000.000], loss: 0.440699, mae: 6.084663, mean_q: -3.122242\n",
      "re_generate cnt=2\n",
      " 13910/50000: episode: 124, duration: 4.120s, episode steps: 143, steps per second: 35, episode reward: -141.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.084 [0.000, 4.000], mean observation: 131.053 [0.000, 1000.000], loss: 0.364135, mae: 6.043674, mean_q: -3.279625\n",
      "re_generate cnt=2\n",
      " 14059/50000: episode: 125, duration: 3.945s, episode steps: 149, steps per second: 38, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.282 [0.000, 4.000], mean observation: 130.239 [0.000, 1000.000], loss: 0.374688, mae: 6.394970, mean_q: -3.410988\n",
      " 14208/50000: episode: 126, duration: 3.949s, episode steps: 149, steps per second: 38, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.168 [0.000, 4.000], mean observation: 127.120 [0.000, 1000.000], loss: 0.362020, mae: 6.730624, mean_q: -3.545994\n",
      " 14256/50000: episode: 127, duration: 2.803s, episode steps: 48, steps per second: 17, episode reward: -46.000, mean reward: -0.958 [-1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 129.023 [0.000, 1000.000], loss: 0.348593, mae: 6.916334, mean_q: -3.671776\n",
      " 14280/50000: episode: 128, duration: 2.516s, episode steps: 24, steps per second: 10, episode reward: -22.000, mean reward: -0.917 [-1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 118.675 [0.000, 1000.000], loss: 0.391600, mae: 6.512206, mean_q: -3.683649\n",
      " 14342/50000: episode: 129, duration: 3.024s, episode steps: 62, steps per second: 21, episode reward: -60.000, mean reward: -0.968 [-1.000, 1.000], mean action: 2.258 [0.000, 4.000], mean observation: 119.923 [0.000, 1000.000], loss: 0.425745, mae: 6.772432, mean_q: -3.716345\n",
      " 14367/50000: episode: 130, duration: 2.500s, episode steps: 25, steps per second: 10, episode reward: -23.000, mean reward: -0.920 [-1.000, 1.000], mean action: 2.240 [0.000, 4.000], mean observation: 121.468 [0.000, 1000.000], loss: 0.393901, mae: 6.775345, mean_q: -3.739434\n",
      " 14381/50000: episode: 131, duration: 2.442s, episode steps: 14, steps per second: 6, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 2.571 [0.000, 4.000], mean observation: 101.386 [0.000, 1000.000], loss: 0.609373, mae: 6.752060, mean_q: -3.738111\n",
      " 14530/50000: episode: 132, duration: 3.791s, episode steps: 149, steps per second: 39, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.007 [0.000, 4.000], mean observation: 127.574 [0.000, 1000.000], loss: 0.723989, mae: 6.402920, mean_q: -3.794104\n",
      " 14679/50000: episode: 133, duration: 3.754s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.302 [0.000, 4.000], mean observation: 133.004 [0.000, 1000.000], loss: 0.422376, mae: 6.318137, mean_q: -3.966410\n",
      " 14828/50000: episode: 134, duration: 3.603s, episode steps: 149, steps per second: 41, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.128 [0.000, 4.000], mean observation: 135.008 [0.000, 1000.000], loss: 0.413245, mae: 6.388904, mean_q: -4.099532\n",
      " 14911/50000: episode: 135, duration: 3.001s, episode steps: 83, steps per second: 28, episode reward: -81.000, mean reward: -0.976 [-1.000, 1.000], mean action: 2.494 [0.000, 4.000], mean observation: 138.400 [0.000, 1000.000], loss: 0.407727, mae: 6.669363, mean_q: -4.200317\n",
      " 15060/50000: episode: 136, duration: 3.713s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.255 [0.000, 4.000], mean observation: 140.679 [0.000, 1000.000], loss: 0.380417, mae: 6.756224, mean_q: -4.322132\n",
      " 15209/50000: episode: 137, duration: 3.532s, episode steps: 149, steps per second: 42, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.597 [0.000, 4.000], mean observation: 133.549 [0.000, 1000.000], loss: 0.415843, mae: 6.899189, mean_q: -4.443688\n",
      " 15294/50000: episode: 138, duration: 3.211s, episode steps: 85, steps per second: 26, episode reward: -83.000, mean reward: -0.976 [-1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 133.384 [0.000, 1000.000], loss: 0.416663, mae: 6.846273, mean_q: -4.538553\n",
      " 15432/50000: episode: 139, duration: 3.551s, episode steps: 138, steps per second: 39, episode reward: -136.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 138.854 [0.000, 1000.000], loss: 0.435378, mae: 6.773976, mean_q: -4.623263\n",
      " 15581/50000: episode: 140, duration: 3.712s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.221 [0.000, 4.000], mean observation: 129.408 [0.000, 1000.000], loss: 0.415293, mae: 6.894507, mean_q: -4.775191\n",
      " 15730/50000: episode: 141, duration: 3.809s, episode steps: 149, steps per second: 39, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.047 [0.000, 4.000], mean observation: 132.448 [0.000, 1000.000], loss: 0.422965, mae: 6.988441, mean_q: -4.895877\n",
      " 15879/50000: episode: 142, duration: 3.723s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.805 [0.000, 4.000], mean observation: 133.317 [0.000, 1000.000], loss: 0.427482, mae: 6.774496, mean_q: -5.014974\n",
      " 15901/50000: episode: 143, duration: 2.643s, episode steps: 22, steps per second: 8, episode reward: -20.000, mean reward: -0.909 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 111.055 [0.000, 1000.000], loss: 0.361890, mae: 6.838060, mean_q: -5.106384\n",
      " 15968/50000: episode: 144, duration: 3.400s, episode steps: 67, steps per second: 20, episode reward: -65.000, mean reward: -0.970 [-1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 139.730 [0.000, 1000.000], loss: 0.377523, mae: 6.803251, mean_q: -5.127593\n",
      " 15976/50000: episode: 145, duration: 2.380s, episode steps: 8, steps per second: 3, episode reward: -6.000, mean reward: -0.750 [-1.000, 1.000], mean action: 2.250 [1.000, 3.000], mean observation: 75.850 [0.000, 1000.000], loss: 0.624795, mae: 6.595440, mean_q: -5.160331\n",
      " 15985/50000: episode: 146, duration: 2.660s, episode steps: 9, steps per second: 3, episode reward: -7.000, mean reward: -0.778 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 89.911 [0.000, 1000.000], loss: 0.335986, mae: 6.644288, mean_q: -5.134061\n",
      " 16033/50000: episode: 147, duration: 3.476s, episode steps: 48, steps per second: 14, episode reward: -46.000, mean reward: -0.958 [-1.000, 1.000], mean action: 2.021 [0.000, 4.000], mean observation: 129.394 [0.000, 1000.000], loss: 0.374650, mae: 7.390774, mean_q: -5.216514\n",
      " 16181/50000: episode: 148, duration: 3.999s, episode steps: 148, steps per second: 37, episode reward: -146.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.196 [0.000, 4.000], mean observation: 134.006 [0.000, 1000.000], loss: 0.521359, mae: 6.958567, mean_q: -5.254216\n",
      " 16196/50000: episode: 149, duration: 2.660s, episode steps: 15, steps per second: 6, episode reward: -13.000, mean reward: -0.867 [-1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 101.567 [0.000, 1000.000], loss: 0.650858, mae: 7.138903, mean_q: -5.355351\n",
      " 16331/50000: episode: 150, duration: 3.699s, episode steps: 135, steps per second: 36, episode reward: -133.000, mean reward: -0.985 [-1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 125.016 [0.000, 1000.000], loss: 0.442630, mae: 6.844893, mean_q: -5.406504\n",
      " 16426/50000: episode: 151, duration: 3.030s, episode steps: 95, steps per second: 31, episode reward: -93.000, mean reward: -0.979 [-1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 122.599 [0.000, 1000.000], loss: 0.611779, mae: 6.809950, mean_q: -5.443380\n",
      " 16575/50000: episode: 152, duration: 4.010s, episode steps: 149, steps per second: 37, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.074 [0.000, 4.000], mean observation: 126.412 [0.000, 1000.000], loss: 0.449220, mae: 6.809056, mean_q: -5.604009\n",
      " 16714/50000: episode: 153, duration: 3.538s, episode steps: 139, steps per second: 39, episode reward: -137.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.223 [0.000, 4.000], mean observation: 134.573 [0.000, 1000.000], loss: 0.491068, mae: 6.873022, mean_q: -5.720922\n",
      " 16863/50000: episode: 154, duration: 3.689s, episode steps: 149, steps per second: 40, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 135.898 [0.000, 1000.000], loss: 0.495706, mae: 6.874858, mean_q: -5.824420\n",
      " 16958/50000: episode: 155, duration: 3.142s, episode steps: 95, steps per second: 30, episode reward: -93.000, mean reward: -0.979 [-1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 132.476 [0.000, 1000.000], loss: 0.496202, mae: 6.776460, mean_q: -5.906494\n",
      " 17107/50000: episode: 156, duration: 3.483s, episode steps: 149, steps per second: 43, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.174 [0.000, 4.000], mean observation: 119.414 [0.000, 1000.000], loss: 0.438879, mae: 6.909504, mean_q: -6.045730\n",
      " 17227/50000: episode: 157, duration: 3.341s, episode steps: 120, steps per second: 36, episode reward: -118.000, mean reward: -0.983 [-1.000, 1.000], mean action: 2.117 [0.000, 4.000], mean observation: 128.749 [0.000, 1000.000], loss: 0.532930, mae: 7.035815, mean_q: -6.140760\n",
      " 17376/50000: episode: 158, duration: 3.564s, episode steps: 149, steps per second: 42, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.336 [0.000, 4.000], mean observation: 125.608 [0.000, 1000.000], loss: 0.544646, mae: 7.112902, mean_q: -6.250274\n",
      " 17525/50000: episode: 159, duration: 3.406s, episode steps: 149, steps per second: 44, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.443 [0.000, 4.000], mean observation: 137.131 [0.000, 1000.000], loss: 0.488587, mae: 7.248832, mean_q: -6.401639\n",
      " 17674/50000: episode: 160, duration: 3.350s, episode steps: 149, steps per second: 44, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.275 [0.000, 4.000], mean observation: 132.056 [0.000, 1000.000], loss: 0.498709, mae: 7.332000, mean_q: -6.520952\n",
      " 17823/50000: episode: 161, duration: 3.444s, episode steps: 149, steps per second: 43, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.436 [0.000, 4.000], mean observation: 131.270 [0.000, 1000.000], loss: 0.536099, mae: 7.331056, mean_q: -6.639156\n",
      " 17824/50000: episode: 162, duration: 2.058s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.000 [0.000, 0.000], loss: 0.306558, mae: 5.727879, mean_q: -6.561239\n",
      " 17825/50000: episode: 163, duration: 2.434s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 1.191927, mae: 9.544670, mean_q: -6.728682\n",
      " 17826/50000: episode: 164, duration: 2.182s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.236847, mae: 7.575905, mean_q: -6.692667\n",
      " 17835/50000: episode: 165, duration: 2.278s, episode steps: 9, steps per second: 4, episode reward: -7.000, mean reward: -0.778 [-1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 134.233 [0.000, 1000.000], loss: 0.283504, mae: 7.295697, mean_q: -6.704561\n",
      " 17843/50000: episode: 166, duration: 2.053s, episode steps: 8, steps per second: 4, episode reward: -6.000, mean reward: -0.750 [-1.000, 1.000], mean action: 1.375 [0.000, 4.000], mean observation: 125.825 [0.000, 1000.000], loss: 0.406271, mae: 7.113416, mean_q: -6.705739\n",
      " 17992/50000: episode: 167, duration: 3.323s, episode steps: 149, steps per second: 45, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.362 [0.000, 4.000], mean observation: 123.487 [0.000, 1000.000], loss: 0.464812, mae: 7.384174, mean_q: -6.782365\n",
      " 17993/50000: episode: 168, duration: 2.070s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.000 [0.000, 0.000], loss: 0.238376, mae: 8.352625, mean_q: -6.892548\n",
      " 17994/50000: episode: 169, duration: 2.079s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.259000, mae: 8.999077, mean_q: -6.859632\n",
      " 17995/50000: episode: 170, duration: 2.066s, episode steps: 1, steps per second: 0, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.000 [0.000, 0.000], loss: 0.231054, mae: 7.711216, mean_q: -6.853851\n",
      " 18000/50000: episode: 171, duration: 1.874s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 2.600 [1.000, 4.000], mean observation: 40.520 [0.000, 1000.000], loss: 0.470181, mae: 7.740037, mean_q: -6.882372\n",
      " 18005/50000: episode: 172, duration: 1.871s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 40.520 [0.000, 1000.000], loss: 1.894327, mae: 7.286703, mean_q: -6.863806\n",
      " 18010/50000: episode: 173, duration: 2.061s, episode steps: 5, steps per second: 2, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 1.000 [0.000, 3.000], mean observation: 40.520 [0.000, 1000.000], loss: 0.639236, mae: 6.952180, mean_q: -6.851762\n",
      " 18159/50000: episode: 174, duration: 3.261s, episode steps: 149, steps per second: 46, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.168 [0.000, 4.000], mean observation: 148.678 [0.000, 1000.000], loss: 0.546006, mae: 7.654865, mean_q: -6.933674\n",
      " 18308/50000: episode: 175, duration: 2.990s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.322 [0.000, 4.000], mean observation: 143.284 [0.000, 1000.000], loss: 0.633516, mae: 7.800010, mean_q: -7.055702\n",
      " 18457/50000: episode: 176, duration: 3.047s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.383 [0.000, 4.000], mean observation: 146.575 [0.000, 1000.000], loss: 0.663664, mae: 7.763502, mean_q: -7.169290\n",
      " 18550/50000: episode: 177, duration: 2.407s, episode steps: 93, steps per second: 39, episode reward: -91.000, mean reward: -0.978 [-1.000, 1.000], mean action: 2.376 [0.000, 4.000], mean observation: 113.185 [0.000, 1000.000], loss: 0.718054, mae: 7.800725, mean_q: -7.240209\n",
      " 18582/50000: episode: 178, duration: 1.923s, episode steps: 32, steps per second: 17, episode reward: -30.000, mean reward: -0.938 [-1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 139.341 [0.000, 1000.000], loss: 0.703066, mae: 7.755350, mean_q: -7.300189\n",
      " 18731/50000: episode: 179, duration: 2.822s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.275 [0.000, 4.000], mean observation: 121.975 [0.000, 1000.000], loss: 0.598782, mae: 7.808997, mean_q: -7.398066\n",
      " 18752/50000: episode: 180, duration: 2.023s, episode steps: 21, steps per second: 10, episode reward: -19.000, mean reward: -0.905 [-1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 92.343 [0.000, 1000.000], loss: 0.587800, mae: 7.695509, mean_q: -7.424201\n",
      " 18807/50000: episode: 181, duration: 2.347s, episode steps: 55, steps per second: 23, episode reward: -53.000, mean reward: -0.964 [-1.000, 1.000], mean action: 1.727 [0.000, 4.000], mean observation: 113.316 [0.000, 1000.000], loss: 0.606267, mae: 7.793610, mean_q: -7.501016\n",
      " 18838/50000: episode: 182, duration: 2.085s, episode steps: 31, steps per second: 15, episode reward: -29.000, mean reward: -0.935 [-1.000, 1.000], mean action: 2.161 [0.000, 4.000], mean observation: 98.810 [0.000, 1000.000], loss: 0.628429, mae: 8.090791, mean_q: -7.545415\n",
      " 18987/50000: episode: 183, duration: 2.927s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.201 [0.000, 4.000], mean observation: 144.183 [0.000, 1000.000], loss: 0.535709, mae: 7.958702, mean_q: -7.611536\n",
      " 19136/50000: episode: 184, duration: 2.944s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.282 [0.000, 4.000], mean observation: 136.168 [0.000, 1000.000], loss: 0.832358, mae: 7.978079, mean_q: -7.722543\n",
      " 19285/50000: episode: 185, duration: 2.931s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.289 [0.000, 4.000], mean observation: 132.175 [0.000, 1000.000], loss: 1.042601, mae: 7.897396, mean_q: -7.740669\n",
      " 19314/50000: episode: 186, duration: 2.037s, episode steps: 29, steps per second: 14, episode reward: -27.000, mean reward: -0.931 [-1.000, 1.000], mean action: 1.931 [0.000, 4.000], mean observation: 129.097 [0.000, 1000.000], loss: 0.733631, mae: 8.032044, mean_q: -7.871142\n",
      " 19341/50000: episode: 187, duration: 2.039s, episode steps: 27, steps per second: 13, episode reward: -25.000, mean reward: -0.926 [-1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 127.407 [0.000, 1000.000], loss: 0.715236, mae: 7.916245, mean_q: -7.925329\n",
      " 19385/50000: episode: 188, duration: 2.229s, episode steps: 44, steps per second: 20, episode reward: -42.000, mean reward: -0.955 [-1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 129.043 [0.000, 1000.000], loss: 0.800990, mae: 8.002102, mean_q: -7.956352\n",
      " 19396/50000: episode: 189, duration: 1.755s, episode steps: 11, steps per second: 6, episode reward: -9.000, mean reward: -0.818 [-1.000, 1.000], mean action: 2.727 [0.000, 4.000], mean observation: 128.600 [0.000, 1000.000], loss: 0.828393, mae: 7.812031, mean_q: -7.974686\n",
      " 19456/50000: episode: 190, duration: 2.129s, episode steps: 60, steps per second: 28, episode reward: -58.000, mean reward: -0.967 [-1.000, 1.000], mean action: 2.383 [0.000, 4.000], mean observation: 125.172 [0.000, 1000.000], loss: 0.666282, mae: 8.053347, mean_q: -8.032628\n",
      " 19581/50000: episode: 191, duration: 2.616s, episode steps: 125, steps per second: 48, episode reward: -123.000, mean reward: -0.984 [-1.000, 1.000], mean action: 2.264 [0.000, 4.000], mean observation: 145.035 [0.000, 1000.000], loss: 0.712254, mae: 8.124681, mean_q: -8.069458\n",
      " 19730/50000: episode: 192, duration: 2.935s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.940 [0.000, 4.000], mean observation: 137.845 [0.000, 1000.000], loss: 0.786566, mae: 8.148937, mean_q: -8.172679\n",
      " 19863/50000: episode: 193, duration: 2.788s, episode steps: 133, steps per second: 48, episode reward: -131.000, mean reward: -0.985 [-1.000, 1.000], mean action: 2.173 [0.000, 4.000], mean observation: 125.629 [0.000, 1000.000], loss: 0.752192, mae: 8.140464, mean_q: -8.314159\n",
      " 20012/50000: episode: 194, duration: 3.054s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.087 [0.000, 4.000], mean observation: 142.052 [0.000, 1000.000], loss: 0.626370, mae: 8.348672, mean_q: -8.438316\n",
      "re_generate cnt=2\n",
      " 20040/50000: episode: 195, duration: 2.032s, episode steps: 28, steps per second: 14, episode reward: -26.000, mean reward: -0.929 [-1.000, 1.000], mean action: 2.321 [0.000, 4.000], mean observation: 137.343 [0.000, 1000.000], loss: 0.795415, mae: 8.176364, mean_q: -8.473042\n",
      "re_generate cnt=2\n",
      " 20091/50000: episode: 196, duration: 2.327s, episode steps: 51, steps per second: 22, episode reward: -49.000, mean reward: -0.961 [-1.000, 1.000], mean action: 2.137 [0.000, 4.000], mean observation: 137.167 [0.000, 1000.000], loss: 0.697169, mae: 8.288134, mean_q: -8.554329\n",
      "re_generate cnt=2\n",
      " 20144/50000: episode: 197, duration: 2.281s, episode steps: 53, steps per second: 23, episode reward: -51.000, mean reward: -0.962 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 152.919 [0.000, 1000.000], loss: 0.854030, mae: 8.239780, mean_q: -8.479297\n",
      " 20149/50000: episode: 198, duration: 1.869s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 3.200 [2.000, 4.000], mean observation: 80.440 [0.000, 1000.000], loss: 0.995027, mae: 8.214392, mean_q: -8.527942\n",
      " 20164/50000: episode: 199, duration: 1.895s, episode steps: 15, steps per second: 8, episode reward: -13.000, mean reward: -0.867 [-1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 108.100 [0.000, 1000.000], loss: 0.663072, mae: 8.311613, mean_q: -8.525957\n",
      " 20183/50000: episode: 200, duration: 1.944s, episode steps: 19, steps per second: 10, episode reward: -17.000, mean reward: -0.895 [-1.000, 1.000], mean action: 1.684 [0.000, 4.000], mean observation: 106.805 [0.000, 1000.000], loss: 0.618939, mae: 8.472095, mean_q: -8.618164\n",
      " 20329/50000: episode: 201, duration: 2.913s, episode steps: 146, steps per second: 50, episode reward: -144.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 124.350 [0.000, 1000.000], loss: 0.804843, mae: 8.389293, mean_q: -8.691577\n",
      " 20478/50000: episode: 202, duration: 2.992s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.121 [0.000, 4.000], mean observation: 109.748 [0.000, 1000.000], loss: 0.912701, mae: 8.477680, mean_q: -8.789667\n",
      " 20627/50000: episode: 203, duration: 3.032s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.054 [0.000, 4.000], mean observation: 109.748 [0.000, 1000.000], loss: 0.746939, mae: 8.505572, mean_q: -8.925557\n",
      " 20776/50000: episode: 204, duration: 3.045s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.040 [0.000, 4.000], mean observation: 120.525 [0.000, 1000.000], loss: 0.735762, mae: 8.551607, mean_q: -9.050516\n",
      " 20925/50000: episode: 205, duration: 2.964s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.094 [0.000, 4.000], mean observation: 122.187 [0.000, 1000.000], loss: 0.917987, mae: 8.488112, mean_q: -9.154652\n",
      " 20953/50000: episode: 206, duration: 2.015s, episode steps: 28, steps per second: 14, episode reward: -26.000, mean reward: -0.929 [-1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 109.818 [0.000, 1000.000], loss: 0.864346, mae: 8.719093, mean_q: -9.279857\n",
      " 21028/50000: episode: 207, duration: 2.406s, episode steps: 75, steps per second: 31, episode reward: -73.000, mean reward: -0.973 [-1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 122.827 [0.000, 1000.000], loss: 0.857555, mae: 8.535151, mean_q: -9.311866\n",
      " 21137/50000: episode: 208, duration: 2.645s, episode steps: 109, steps per second: 41, episode reward: -107.000, mean reward: -0.982 [-1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 140.011 [0.000, 1000.000], loss: 0.835763, mae: 8.551730, mean_q: -9.372777\n",
      " 21172/50000: episode: 209, duration: 2.087s, episode steps: 35, steps per second: 17, episode reward: -33.000, mean reward: -0.943 [-1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 139.491 [0.000, 1000.000], loss: 0.988552, mae: 8.561586, mean_q: -9.418505\n",
      " 21321/50000: episode: 210, duration: 2.897s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.148 [0.000, 4.000], mean observation: 144.685 [0.000, 1000.000], loss: 0.816714, mae: 8.686581, mean_q: -9.487633\n",
      " 21344/50000: episode: 211, duration: 1.939s, episode steps: 23, steps per second: 12, episode reward: -21.000, mean reward: -0.913 [-1.000, 1.000], mean action: 2.957 [0.000, 4.000], mean observation: 119.291 [0.000, 1000.000], loss: 0.744980, mae: 8.718561, mean_q: -9.580488\n",
      " 21461/50000: episode: 212, duration: 2.576s, episode steps: 117, steps per second: 45, episode reward: -115.000, mean reward: -0.983 [-1.000, 1.000], mean action: 2.137 [0.000, 4.000], mean observation: 140.246 [0.000, 1000.000], loss: 1.016934, mae: 8.824494, mean_q: -9.568418\n",
      " 21610/50000: episode: 213, duration: 3.038s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.121 [0.000, 4.000], mean observation: 128.383 [0.000, 1000.000], loss: 1.031671, mae: 8.706826, mean_q: -9.652841\n",
      " 21743/50000: episode: 214, duration: 2.818s, episode steps: 133, steps per second: 47, episode reward: -131.000, mean reward: -0.985 [-1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 140.606 [0.000, 1000.000], loss: 0.999182, mae: 8.740267, mean_q: -9.837545\n",
      " 21892/50000: episode: 215, duration: 2.951s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.993 [0.000, 4.000], mean observation: 145.068 [0.000, 1000.000], loss: 1.007406, mae: 8.833220, mean_q: -9.916442\n",
      " 22041/50000: episode: 216, duration: 2.989s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.168 [0.000, 4.000], mean observation: 132.033 [0.000, 1000.000], loss: 1.177432, mae: 8.840639, mean_q: -9.971006\n",
      " 22175/50000: episode: 217, duration: 2.837s, episode steps: 134, steps per second: 47, episode reward: -132.000, mean reward: -0.985 [-1.000, 1.000], mean action: 2.134 [0.000, 4.000], mean observation: 131.786 [0.000, 1000.000], loss: 1.017248, mae: 8.816182, mean_q: -10.070606\n",
      " 22324/50000: episode: 218, duration: 3.075s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 137.320 [0.000, 1000.000], loss: 1.099180, mae: 8.857931, mean_q: -10.184956\n",
      " 22377/50000: episode: 219, duration: 2.251s, episode steps: 53, steps per second: 24, episode reward: -51.000, mean reward: -0.962 [-1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 126.462 [0.000, 1000.000], loss: 1.069670, mae: 8.823591, mean_q: -10.318089\n",
      " 22484/50000: episode: 220, duration: 2.577s, episode steps: 107, steps per second: 42, episode reward: -105.000, mean reward: -0.981 [-1.000, 1.000], mean action: 1.916 [0.000, 4.000], mean observation: 132.317 [0.000, 1000.000], loss: 1.070786, mae: 8.855853, mean_q: -10.341378\n",
      " 22504/50000: episode: 221, duration: 1.910s, episode steps: 20, steps per second: 10, episode reward: -18.000, mean reward: -0.900 [-1.000, 1.000], mean action: 1.100 [0.000, 3.000], mean observation: 121.210 [0.000, 1000.000], loss: 0.985188, mae: 8.841809, mean_q: -10.412746\n",
      " 22653/50000: episode: 222, duration: 2.826s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.013 [0.000, 4.000], mean observation: 123.809 [0.000, 1000.000], loss: 1.181031, mae: 8.882934, mean_q: -10.474263\n",
      " 22802/50000: episode: 223, duration: 2.805s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.027 [0.000, 4.000], mean observation: 130.312 [0.000, 1000.000], loss: 1.002374, mae: 8.975717, mean_q: -10.568703\n",
      " 22951/50000: episode: 224, duration: 2.817s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.074 [0.000, 4.000], mean observation: 135.374 [0.000, 1000.000], loss: 0.969321, mae: 8.984606, mean_q: -10.677398\n",
      " 23072/50000: episode: 225, duration: 2.772s, episode steps: 121, steps per second: 44, episode reward: -119.000, mean reward: -0.983 [-1.000, 1.000], mean action: 2.264 [0.000, 4.000], mean observation: 148.127 [0.000, 1000.000], loss: 0.997376, mae: 9.046114, mean_q: -10.797747\n",
      " 23221/50000: episode: 226, duration: 2.992s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.758 [0.000, 4.000], mean observation: 145.270 [0.000, 1000.000], loss: 1.009766, mae: 9.097424, mean_q: -10.906982\n",
      " 23337/50000: episode: 227, duration: 2.724s, episode steps: 116, steps per second: 43, episode reward: -114.000, mean reward: -0.983 [-1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 145.222 [0.000, 1000.000], loss: 1.143945, mae: 9.111453, mean_q: -10.980801\n",
      " 23356/50000: episode: 228, duration: 1.851s, episode steps: 19, steps per second: 10, episode reward: -17.000, mean reward: -0.895 [-1.000, 1.000], mean action: 2.158 [0.000, 4.000], mean observation: 112.463 [0.000, 1000.000], loss: 0.758306, mae: 9.133675, mean_q: -11.055314\n",
      " 23416/50000: episode: 229, duration: 2.144s, episode steps: 60, steps per second: 28, episode reward: -58.000, mean reward: -0.967 [-1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 143.118 [0.000, 1000.000], loss: 0.904075, mae: 9.171276, mean_q: -11.075971\n",
      " 23446/50000: episode: 230, duration: 1.920s, episode steps: 30, steps per second: 16, episode reward: -28.000, mean reward: -0.933 [-1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 129.297 [0.000, 1000.000], loss: 1.003997, mae: 9.191348, mean_q: -11.097751\n",
      " 23457/50000: episode: 231, duration: 1.916s, episode steps: 11, steps per second: 6, episode reward: -9.000, mean reward: -0.818 [-1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 119.236 [0.000, 1000.000], loss: 1.466504, mae: 9.198553, mean_q: -11.101915\n",
      " 23475/50000: episode: 232, duration: 1.955s, episode steps: 18, steps per second: 9, episode reward: -16.000, mean reward: -0.889 [-1.000, 1.000], mean action: 3.167 [0.000, 4.000], mean observation: 90.094 [0.000, 1000.000], loss: 0.994130, mae: 9.180613, mean_q: -11.102839\n",
      " 23486/50000: episode: 233, duration: 1.919s, episode steps: 11, steps per second: 6, episode reward: -9.000, mean reward: -0.818 [-1.000, 1.000], mean action: 3.000 [0.000, 4.000], mean observation: 82.836 [0.000, 1000.000], loss: 0.625302, mae: 9.172391, mean_q: -11.088610\n",
      " 23635/50000: episode: 234, duration: 3.076s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.208 [0.000, 4.000], mean observation: 141.876 [0.000, 1000.000], loss: 1.051488, mae: 9.233298, mean_q: -11.153784\n",
      " 23784/50000: episode: 235, duration: 2.983s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.087 [0.000, 4.000], mean observation: 149.215 [0.000, 1000.000], loss: 1.122284, mae: 9.322862, mean_q: -11.273252\n",
      " 23810/50000: episode: 236, duration: 2.053s, episode steps: 26, steps per second: 13, episode reward: -24.000, mean reward: -0.923 [-1.000, 1.000], mean action: 1.885 [0.000, 4.000], mean observation: 136.738 [0.000, 1000.000], loss: 0.843285, mae: 9.362459, mean_q: -11.328857\n",
      " 23959/50000: episode: 237, duration: 2.956s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.034 [0.000, 4.000], mean observation: 119.087 [0.000, 1000.000], loss: 1.029823, mae: 9.415989, mean_q: -11.377161\n",
      " 24108/50000: episode: 238, duration: 2.965s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.040 [0.000, 4.000], mean observation: 129.993 [0.000, 1000.000], loss: 1.055975, mae: 9.499829, mean_q: -11.481338\n",
      " 24168/50000: episode: 239, duration: 2.243s, episode steps: 60, steps per second: 27, episode reward: -58.000, mean reward: -0.967 [-1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 120.335 [0.000, 1000.000], loss: 0.926747, mae: 9.555638, mean_q: -11.571977\n",
      " 24317/50000: episode: 240, duration: 3.064s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.678 [0.000, 4.000], mean observation: 141.568 [0.000, 1000.000], loss: 0.928739, mae: 9.615737, mean_q: -11.648956\n",
      " 24466/50000: episode: 241, duration: 3.018s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.987 [0.000, 4.000], mean observation: 139.948 [0.000, 1000.000], loss: 0.926103, mae: 9.701838, mean_q: -11.767658\n",
      " 24615/50000: episode: 242, duration: 3.086s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.074 [0.000, 4.000], mean observation: 147.655 [0.000, 1000.000], loss: 1.005306, mae: 9.793118, mean_q: -11.876163\n",
      " 24764/50000: episode: 243, duration: 3.009s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.953 [0.000, 4.000], mean observation: 140.540 [0.000, 1000.000], loss: 0.970118, mae: 9.875855, mean_q: -11.976694\n",
      " 24913/50000: episode: 244, duration: 2.999s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.054 [0.000, 4.000], mean observation: 140.782 [0.000, 1000.000], loss: 0.810826, mae: 9.960912, mean_q: -12.093433\n",
      " 25062/50000: episode: 245, duration: 2.958s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.114 [0.000, 4.000], mean observation: 141.702 [0.000, 1000.000], loss: 0.985397, mae: 10.058608, mean_q: -12.228582\n",
      " 25211/50000: episode: 246, duration: 3.037s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.141 [0.000, 4.000], mean observation: 128.898 [0.000, 1000.000], loss: 1.039100, mae: 10.141064, mean_q: -12.321332\n",
      " 25360/50000: episode: 247, duration: 2.988s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.007 [0.000, 4.000], mean observation: 137.407 [0.000, 1000.000], loss: 1.055784, mae: 10.228106, mean_q: -12.430993\n",
      " 25509/50000: episode: 248, duration: 3.049s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.040 [0.000, 4.000], mean observation: 129.551 [0.000, 1000.000], loss: 0.957317, mae: 10.312016, mean_q: -12.540559\n",
      " 25557/50000: episode: 249, duration: 2.130s, episode steps: 48, steps per second: 23, episode reward: -46.000, mean reward: -0.958 [-1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 146.002 [0.000, 1000.000], loss: 0.834122, mae: 10.365035, mean_q: -12.616597\n",
      " 25585/50000: episode: 250, duration: 1.918s, episode steps: 28, steps per second: 15, episode reward: -26.000, mean reward: -0.929 [-1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 141.179 [0.000, 1000.000], loss: 1.161415, mae: 10.402307, mean_q: -12.653364\n",
      " 25643/50000: episode: 251, duration: 2.130s, episode steps: 58, steps per second: 27, episode reward: -56.000, mean reward: -0.966 [-1.000, 1.000], mean action: 1.879 [0.000, 4.000], mean observation: 143.591 [0.000, 1000.000], loss: 1.114440, mae: 10.424783, mean_q: -12.677288\n",
      " 25792/50000: episode: 252, duration: 2.871s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.919 [0.000, 4.000], mean observation: 137.197 [0.000, 1000.000], loss: 1.142527, mae: 10.489819, mean_q: -12.715922\n",
      " 25941/50000: episode: 253, duration: 2.860s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.940 [0.000, 4.000], mean observation: 135.097 [0.000, 1000.000], loss: 1.148022, mae: 10.592592, mean_q: -12.772400\n",
      " 26090/50000: episode: 254, duration: 2.886s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.852 [0.000, 4.000], mean observation: 136.715 [0.000, 1000.000], loss: 0.947791, mae: 10.640837, mean_q: -12.936268\n",
      " 26239/50000: episode: 255, duration: 2.921s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.081 [0.000, 4.000], mean observation: 124.954 [0.000, 1000.000], loss: 1.033149, mae: 10.732897, mean_q: -13.043824\n",
      " 26301/50000: episode: 256, duration: 2.295s, episode steps: 62, steps per second: 27, episode reward: -60.000, mean reward: -0.968 [-1.000, 1.000], mean action: 2.290 [0.000, 4.000], mean observation: 120.831 [0.000, 1000.000], loss: 1.096384, mae: 10.794703, mean_q: -13.103975\n",
      " 26371/50000: episode: 257, duration: 2.436s, episode steps: 70, steps per second: 29, episode reward: -68.000, mean reward: -0.971 [-1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 124.437 [0.000, 1000.000], loss: 1.038357, mae: 10.837329, mean_q: -13.216117\n",
      " 26379/50000: episode: 258, duration: 1.968s, episode steps: 8, steps per second: 4, episode reward: -6.000, mean reward: -0.750 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 113.525 [0.000, 1000.000], loss: 0.703190, mae: 10.849674, mean_q: -13.241766\n",
      " 26460/50000: episode: 259, duration: 2.553s, episode steps: 81, steps per second: 32, episode reward: -79.000, mean reward: -0.975 [-1.000, 1.000], mean action: 1.951 [0.000, 4.000], mean observation: 139.809 [0.000, 1000.000], loss: 1.176050, mae: 10.891294, mean_q: -13.269649\n",
      " 26468/50000: episode: 260, duration: 1.979s, episode steps: 8, steps per second: 4, episode reward: -6.000, mean reward: -0.750 [-1.000, 1.000], mean action: 2.875 [0.000, 4.000], mean observation: 113.525 [0.000, 1000.000], loss: 0.655719, mae: 10.901608, mean_q: -13.273726\n",
      " 26483/50000: episode: 261, duration: 2.034s, episode steps: 15, steps per second: 7, episode reward: -13.000, mean reward: -0.867 [-1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 34.633 [0.000, 1000.000], loss: 1.136920, mae: 10.923900, mean_q: -13.331332\n",
      " 26498/50000: episode: 262, duration: 2.046s, episode steps: 15, steps per second: 7, episode reward: -13.000, mean reward: -0.867 [-1.000, 1.000], mean action: 1.467 [0.000, 4.000], mean observation: 34.633 [0.000, 1000.000], loss: 1.233111, mae: 10.918469, mean_q: -13.167150\n",
      " 26513/50000: episode: 263, duration: 2.040s, episode steps: 15, steps per second: 7, episode reward: -13.000, mean reward: -0.867 [-1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 34.633 [0.000, 1000.000], loss: 1.264700, mae: 10.932192, mean_q: -13.269061\n",
      " 26639/50000: episode: 264, duration: 2.625s, episode steps: 126, steps per second: 48, episode reward: -124.000, mean reward: -0.984 [-1.000, 1.000], mean action: 1.476 [0.000, 4.000], mean observation: 102.076 [0.000, 1000.000], loss: 1.305169, mae: 10.977704, mean_q: -13.272415\n",
      " 26788/50000: episode: 265, duration: 2.885s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 129.825 [0.000, 1000.000], loss: 1.324526, mae: 11.040721, mean_q: -13.247456\n",
      " 26937/50000: episode: 266, duration: 2.825s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.745 [0.000, 4.000], mean observation: 102.765 [0.000, 1000.000], loss: 1.260030, mae: 11.115696, mean_q: -13.314143\n",
      " 26985/50000: episode: 267, duration: 2.182s, episode steps: 48, steps per second: 22, episode reward: -46.000, mean reward: -0.958 [-1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 145.775 [0.000, 1000.000], loss: 1.039903, mae: 11.151456, mean_q: -13.336652\n",
      " 27110/50000: episode: 268, duration: 2.778s, episode steps: 125, steps per second: 45, episode reward: -123.000, mean reward: -0.984 [-1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 120.854 [0.000, 1000.000], loss: 1.079454, mae: 11.199403, mean_q: -13.384592\n",
      " 27218/50000: episode: 269, duration: 2.638s, episode steps: 108, steps per second: 41, episode reward: -106.000, mean reward: -0.981 [-1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 140.173 [0.000, 1000.000], loss: 0.946183, mae: 11.258323, mean_q: -13.500671\n",
      " 27254/50000: episode: 270, duration: 2.099s, episode steps: 36, steps per second: 17, episode reward: -34.000, mean reward: -0.944 [-1.000, 1.000], mean action: 1.944 [0.000, 4.000], mean observation: 118.317 [0.000, 1000.000], loss: 0.881396, mae: 11.292857, mean_q: -13.555856\n",
      " 27283/50000: episode: 271, duration: 2.054s, episode steps: 29, steps per second: 14, episode reward: -27.000, mean reward: -0.931 [-1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 101.710 [0.000, 1000.000], loss: 1.011754, mae: 11.321744, mean_q: -13.611918\n",
      " 27299/50000: episode: 272, duration: 2.014s, episode steps: 16, steps per second: 8, episode reward: -14.000, mean reward: -0.875 [-1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 95.156 [0.000, 1000.000], loss: 0.601099, mae: 11.325247, mean_q: -13.679708\n",
      " 27448/50000: episode: 273, duration: 2.952s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.993 [0.000, 4.000], mean observation: 128.142 [0.000, 1000.000], loss: 1.208068, mae: 11.380705, mean_q: -13.672254\n",
      " 27597/50000: episode: 274, duration: 3.001s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 118.495 [0.000, 1000.000], loss: 1.248737, mae: 11.467445, mean_q: -13.848072\n",
      " 27692/50000: episode: 275, duration: 2.551s, episode steps: 95, steps per second: 37, episode reward: -93.000, mean reward: -0.979 [-1.000, 1.000], mean action: 1.916 [0.000, 4.000], mean observation: 132.459 [0.000, 1000.000], loss: 1.467111, mae: 11.532379, mean_q: -13.893727\n",
      " 27841/50000: episode: 276, duration: 2.882s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.866 [0.000, 4.000], mean observation: 151.113 [0.000, 1000.000], loss: 1.154225, mae: 11.589980, mean_q: -14.043916\n",
      " 27990/50000: episode: 277, duration: 2.854s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 148.207 [0.000, 1000.000], loss: 1.529835, mae: 11.678010, mean_q: -14.127989\n",
      " 28139/50000: episode: 278, duration: 2.888s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.356 [0.000, 4.000], mean observation: 156.605 [0.000, 1000.000], loss: 1.237515, mae: 11.752034, mean_q: -14.285076\n",
      " 28215/50000: episode: 279, duration: 2.470s, episode steps: 76, steps per second: 31, episode reward: -74.000, mean reward: -0.974 [-1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 149.882 [0.000, 1000.000], loss: 1.060075, mae: 11.810214, mean_q: -14.351428\n",
      " 28364/50000: episode: 280, duration: 3.106s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.174 [0.000, 4.000], mean observation: 144.654 [0.000, 1000.000], loss: 1.449962, mae: 11.881186, mean_q: -14.421337\n",
      " 28513/50000: episode: 281, duration: 3.027s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.101 [0.000, 4.000], mean observation: 148.343 [0.000, 1000.000], loss: 1.479517, mae: 11.960436, mean_q: -14.572007\n",
      " 28601/50000: episode: 282, duration: 2.627s, episode steps: 88, steps per second: 33, episode reward: -86.000, mean reward: -0.977 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 140.527 [0.000, 1000.000], loss: 0.974614, mae: 12.009528, mean_q: -14.665064\n",
      " 28750/50000: episode: 283, duration: 3.080s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.993 [0.000, 4.000], mean observation: 137.731 [0.000, 1000.000], loss: 1.382956, mae: 12.082133, mean_q: -14.695689\n",
      " 28899/50000: episode: 284, duration: 3.106s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.188 [0.000, 4.000], mean observation: 135.344 [0.000, 1000.000], loss: 1.335647, mae: 12.151347, mean_q: -14.689380\n",
      " 29005/50000: episode: 285, duration: 2.649s, episode steps: 106, steps per second: 40, episode reward: -104.000, mean reward: -0.981 [-1.000, 1.000], mean action: 2.085 [0.000, 4.000], mean observation: 138.658 [0.000, 1000.000], loss: 1.117929, mae: 12.200282, mean_q: -14.646169\n",
      " 29154/50000: episode: 286, duration: 3.011s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.221 [0.000, 4.000], mean observation: 144.548 [0.000, 1000.000], loss: 1.182976, mae: 12.231178, mean_q: -14.375454\n",
      " 29303/50000: episode: 287, duration: 2.989s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.020 [0.000, 4.000], mean observation: 156.242 [0.000, 1000.000], loss: 1.533538, mae: 12.350587, mean_q: -14.182658\n",
      "re_generate cnt=2\n",
      " 29452/50000: episode: 288, duration: 3.027s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 138.352 [0.000, 1000.000], loss: 1.146258, mae: 12.414095, mean_q: -14.321414\n",
      "re_generate cnt=2\n",
      " 29514/50000: episode: 289, duration: 2.328s, episode steps: 62, steps per second: 27, episode reward: -60.000, mean reward: -0.968 [-1.000, 1.000], mean action: 2.081 [0.000, 4.000], mean observation: 129.298 [0.000, 1000.000], loss: 1.118410, mae: 12.450624, mean_q: -14.481455\n",
      "re_generate cnt=2\n",
      " 29663/50000: episode: 290, duration: 3.048s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.007 [0.000, 4.000], mean observation: 142.188 [0.000, 1000.000], loss: 1.518525, mae: 12.476016, mean_q: -14.597035\n",
      " 29810/50000: episode: 291, duration: 3.142s, episode steps: 147, steps per second: 47, episode reward: -145.000, mean reward: -0.986 [-1.000, 1.000], mean action: 1.993 [0.000, 4.000], mean observation: 139.429 [0.000, 1000.000], loss: 1.592938, mae: 12.558254, mean_q: -14.727006\n",
      " 29878/50000: episode: 292, duration: 2.457s, episode steps: 68, steps per second: 28, episode reward: -66.000, mean reward: -0.971 [-1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 135.734 [0.000, 1000.000], loss: 1.939397, mae: 12.621179, mean_q: -14.707483\n",
      " 29894/50000: episode: 293, duration: 2.124s, episode steps: 16, steps per second: 8, episode reward: -14.000, mean reward: -0.875 [-1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 120.362 [0.000, 1000.000], loss: 0.727460, mae: 12.594632, mean_q: -14.603693\n",
      " 29899/50000: episode: 294, duration: 1.853s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: 80.640 [0.000, 1000.000], loss: 0.930297, mae: 12.604274, mean_q: -14.603895\n",
      " 29908/50000: episode: 295, duration: 1.929s, episode steps: 9, steps per second: 5, episode reward: -7.000, mean reward: -0.778 [-1.000, 1.000], mean action: 2.667 [1.000, 4.000], mean observation: 67.444 [0.000, 1000.000], loss: 1.493156, mae: 12.613166, mean_q: -14.557200\n",
      " 29917/50000: episode: 296, duration: 1.861s, episode steps: 9, steps per second: 5, episode reward: -7.000, mean reward: -0.778 [-1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 67.444 [0.000, 1000.000], loss: 1.667149, mae: 12.635326, mean_q: -14.683577\n",
      " 30066/50000: episode: 297, duration: 3.023s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.000 [0.000, 4.000], mean observation: 140.726 [0.000, 1000.000], loss: 1.755126, mae: 12.700757, mean_q: -14.944194\n",
      " 30208/50000: episode: 298, duration: 2.975s, episode steps: 142, steps per second: 48, episode reward: -140.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 139.756 [0.000, 1000.000], loss: 1.252193, mae: 12.766779, mean_q: -15.020064\n",
      " 30357/50000: episode: 299, duration: 2.984s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.168 [0.000, 4.000], mean observation: 132.514 [0.000, 1000.000], loss: 1.519731, mae: 12.868798, mean_q: -15.263170\n",
      " 30401/50000: episode: 300, duration: 2.204s, episode steps: 44, steps per second: 20, episode reward: -42.000, mean reward: -0.955 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 133.361 [0.000, 1000.000], loss: 1.580655, mae: 12.932330, mean_q: -15.424384\n",
      " 30489/50000: episode: 301, duration: 2.528s, episode steps: 88, steps per second: 35, episode reward: -86.000, mean reward: -0.977 [-1.000, 1.000], mean action: 1.989 [0.000, 4.000], mean observation: 132.485 [0.000, 1000.000], loss: 1.485678, mae: 12.949929, mean_q: -15.377959\n",
      " 30638/50000: episode: 302, duration: 3.019s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.926 [0.000, 4.000], mean observation: 131.676 [0.000, 1000.000], loss: 1.607419, mae: 13.023715, mean_q: -15.541547\n",
      " 30772/50000: episode: 303, duration: 3.032s, episode steps: 134, steps per second: 44, episode reward: -132.000, mean reward: -0.985 [-1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 135.308 [0.000, 1000.000], loss: 1.341954, mae: 13.089197, mean_q: -15.628280\n",
      " 30797/50000: episode: 304, duration: 2.011s, episode steps: 25, steps per second: 12, episode reward: -23.000, mean reward: -0.920 [-1.000, 1.000], mean action: 1.960 [0.000, 4.000], mean observation: 109.896 [0.000, 1000.000], loss: 1.583317, mae: 13.139480, mean_q: -15.632206\n",
      " 30837/50000: episode: 305, duration: 2.114s, episode steps: 40, steps per second: 19, episode reward: -38.000, mean reward: -0.950 [-1.000, 1.000], mean action: 1.925 [0.000, 4.000], mean observation: 130.102 [0.000, 1000.000], loss: 1.381687, mae: 13.157217, mean_q: -15.764468\n",
      " 30964/50000: episode: 306, duration: 2.712s, episode steps: 127, steps per second: 47, episode reward: -125.000, mean reward: -0.984 [-1.000, 1.000], mean action: 1.913 [0.000, 4.000], mean observation: 129.988 [0.000, 1000.000], loss: 1.837905, mae: 13.219934, mean_q: -15.881557\n",
      " 31113/50000: episode: 307, duration: 2.845s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.946 [0.000, 4.000], mean observation: 137.181 [0.000, 1000.000], loss: 1.599665, mae: 13.273252, mean_q: -15.940909\n",
      " 31129/50000: episode: 308, duration: 1.818s, episode steps: 16, steps per second: 9, episode reward: -14.000, mean reward: -0.875 [-1.000, 1.000], mean action: 1.438 [0.000, 4.000], mean observation: 151.806 [0.000, 1000.000], loss: 1.020550, mae: 13.298312, mean_q: -15.999916\n",
      " 31278/50000: episode: 309, duration: 3.037s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.027 [0.000, 4.000], mean observation: 138.993 [0.000, 1000.000], loss: 1.434359, mae: 13.342650, mean_q: -15.921270\n",
      " 31427/50000: episode: 310, duration: 3.001s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.114 [0.000, 4.000], mean observation: 135.081 [0.000, 1000.000], loss: 1.535267, mae: 13.389539, mean_q: -15.768799\n",
      " 31576/50000: episode: 311, duration: 2.940s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.188 [0.000, 4.000], mean observation: 141.577 [0.000, 1000.000], loss: 1.508613, mae: 13.476192, mean_q: -16.019276\n",
      " 31604/50000: episode: 312, duration: 2.029s, episode steps: 28, steps per second: 14, episode reward: -26.000, mean reward: -0.929 [-1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 109.879 [0.000, 1000.000], loss: 2.055465, mae: 13.570624, mean_q: -16.397444\n",
      " 31727/50000: episode: 313, duration: 2.749s, episode steps: 123, steps per second: 45, episode reward: -121.000, mean reward: -0.984 [-1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 144.648 [0.000, 1000.000], loss: 1.731312, mae: 13.594888, mean_q: -16.382681\n",
      " 31781/50000: episode: 314, duration: 2.209s, episode steps: 54, steps per second: 24, episode reward: -52.000, mean reward: -0.963 [-1.000, 1.000], mean action: 1.907 [0.000, 4.000], mean observation: 131.231 [0.000, 1000.000], loss: 1.453900, mae: 13.631902, mean_q: -16.428905\n",
      " 31783/50000: episode: 315, duration: 1.682s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 50.500 [0.000, 1000.000], loss: 1.537405, mae: 13.635992, mean_q: -16.290005\n",
      " 31785/50000: episode: 316, duration: 1.688s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 1.000 [0.000, 2.000], mean observation: 50.500 [0.000, 1000.000], loss: 2.748497, mae: 13.677717, mean_q: -16.500488\n",
      " 31805/50000: episode: 317, duration: 1.828s, episode steps: 20, steps per second: 11, episode reward: -18.000, mean reward: -0.900 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 115.925 [0.000, 1000.000], loss: 1.774067, mae: 13.686064, mean_q: -16.624182\n",
      " 31954/50000: episode: 318, duration: 2.942s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.859 [0.000, 4.000], mean observation: 146.675 [0.000, 1000.000], loss: 1.991082, mae: 13.743993, mean_q: -16.822271\n",
      " 32003/50000: episode: 319, duration: 2.094s, episode steps: 49, steps per second: 23, episode reward: -47.000, mean reward: -0.959 [-1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 137.143 [0.000, 1000.000], loss: 1.692629, mae: 13.793294, mean_q: -16.954189\n",
      " 32047/50000: episode: 320, duration: 2.065s, episode steps: 44, steps per second: 21, episode reward: -42.000, mean reward: -0.955 [-1.000, 1.000], mean action: 2.432 [0.000, 4.000], mean observation: 129.770 [0.000, 1000.000], loss: 1.409014, mae: 13.808270, mean_q: -16.972506\n",
      " 32052/50000: episode: 321, duration: 1.865s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 3.400 [2.000, 4.000], mean observation: 60.600 [0.000, 1000.000], loss: 2.111084, mae: 13.844255, mean_q: -17.009272\n",
      " 32057/50000: episode: 322, duration: 1.863s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 60.600 [0.000, 1000.000], loss: 3.614216, mae: 13.875104, mean_q: -16.954603\n",
      " 32062/50000: episode: 323, duration: 1.864s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 60.600 [0.000, 1000.000], loss: 1.608026, mae: 13.827616, mean_q: -16.950556\n",
      " 32096/50000: episode: 324, duration: 2.170s, episode steps: 34, steps per second: 16, episode reward: -32.000, mean reward: -0.941 [-1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 134.062 [0.000, 1000.000], loss: 2.201274, mae: 13.854350, mean_q: -16.979216\n",
      " 32115/50000: episode: 325, duration: 2.037s, episode steps: 19, steps per second: 9, episode reward: -17.000, mean reward: -0.895 [-1.000, 1.000], mean action: 2.579 [0.000, 4.000], mean observation: 154.247 [0.000, 1000.000], loss: 1.820191, mae: 13.857257, mean_q: -17.032337\n",
      " 32145/50000: episode: 326, duration: 2.185s, episode steps: 30, steps per second: 14, episode reward: -28.000, mean reward: -0.933 [-1.000, 1.000], mean action: 1.633 [0.000, 4.000], mean observation: 125.167 [0.000, 1000.000], loss: 2.295842, mae: 13.878884, mean_q: -17.008995\n",
      " 32221/50000: episode: 327, duration: 2.387s, episode steps: 76, steps per second: 32, episode reward: -74.000, mean reward: -0.974 [-1.000, 1.000], mean action: 1.842 [0.000, 4.000], mean observation: 98.983 [0.000, 1000.000], loss: 2.040675, mae: 13.896436, mean_q: -17.024080\n",
      " 32253/50000: episode: 328, duration: 2.043s, episode steps: 32, steps per second: 16, episode reward: -30.000, mean reward: -0.938 [-1.000, 1.000], mean action: 1.594 [0.000, 4.000], mean observation: 108.650 [0.000, 1000.000], loss: 1.588891, mae: 13.916479, mean_q: -17.113161\n",
      " 32273/50000: episode: 329, duration: 1.958s, episode steps: 20, steps per second: 10, episode reward: -18.000, mean reward: -0.900 [-1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 66.795 [0.000, 1000.000], loss: 1.244431, mae: 13.921481, mean_q: -17.052641\n",
      " 32422/50000: episode: 330, duration: 2.979s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.188 [0.000, 4.000], mean observation: 137.352 [0.000, 1000.000], loss: 2.349588, mae: 13.986423, mean_q: -17.112991\n",
      " 32484/50000: episode: 331, duration: 2.294s, episode steps: 62, steps per second: 27, episode reward: -60.000, mean reward: -0.968 [-1.000, 1.000], mean action: 2.161 [0.000, 4.000], mean observation: 136.908 [0.000, 1000.000], loss: 2.194686, mae: 14.020353, mean_q: -17.176359\n",
      " 32633/50000: episode: 332, duration: 3.010s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.893 [0.000, 4.000], mean observation: 137.377 [0.000, 1000.000], loss: 1.974849, mae: 14.063310, mean_q: -17.205328\n",
      " 32639/50000: episode: 333, duration: 1.946s, episode steps: 6, steps per second: 3, episode reward: -4.000, mean reward: -0.667 [-1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: 84.133 [0.000, 1000.000], loss: 2.401122, mae: 14.115909, mean_q: -17.275936\n",
      " 32728/50000: episode: 334, duration: 2.508s, episode steps: 89, steps per second: 35, episode reward: -87.000, mean reward: -0.978 [-1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 143.765 [0.000, 1000.000], loss: 2.060373, mae: 14.123231, mean_q: -17.300714\n",
      " 32763/50000: episode: 335, duration: 2.098s, episode steps: 35, steps per second: 17, episode reward: -33.000, mean reward: -0.943 [-1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 127.686 [0.000, 1000.000], loss: 2.559878, mae: 14.157924, mean_q: -17.303404\n",
      " 32765/50000: episode: 336, duration: 1.863s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 50.700 [0.000, 1000.000], loss: 2.934373, mae: 14.180634, mean_q: -17.401794\n",
      " 32767/50000: episode: 337, duration: 1.947s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 2.500 [1.000, 4.000], mean observation: 50.700 [0.000, 1000.000], loss: 0.252190, mae: 14.117870, mean_q: -17.395218\n",
      " 32769/50000: episode: 338, duration: 1.849s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 1.500 [1.000, 2.000], mean observation: 50.700 [0.000, 1000.000], loss: 4.457655, mae: 14.222122, mean_q: -17.405005\n",
      " 32918/50000: episode: 339, duration: 2.959s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.866 [0.000, 4.000], mean observation: 136.254 [0.000, 1000.000], loss: 2.431013, mae: 14.202994, mean_q: -17.378611\n",
      " 33067/50000: episode: 340, duration: 2.979s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.007 [0.000, 4.000], mean observation: 135.589 [0.000, 1000.000], loss: 2.051322, mae: 14.248724, mean_q: -17.445768\n",
      " 33080/50000: episode: 341, duration: 1.985s, episode steps: 13, steps per second: 7, episode reward: -11.000, mean reward: -0.846 [-1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 109.131 [0.000, 1000.000], loss: 3.517899, mae: 14.312154, mean_q: -17.483244\n",
      " 33229/50000: episode: 342, duration: 2.821s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.933 [0.000, 4.000], mean observation: 141.212 [0.000, 1000.000], loss: 2.400051, mae: 14.316370, mean_q: -17.513842\n",
      " 33378/50000: episode: 343, duration: 2.808s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.893 [0.000, 4.000], mean observation: 141.613 [0.000, 1000.000], loss: 2.392765, mae: 14.373568, mean_q: -17.556742\n",
      " 33409/50000: episode: 344, duration: 1.921s, episode steps: 31, steps per second: 16, episode reward: -29.000, mean reward: -0.935 [-1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 125.006 [0.000, 1000.000], loss: 1.828273, mae: 14.388997, mean_q: -17.620543\n",
      " 33410/50000: episode: 345, duration: 1.635s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 4.947729, mae: 14.494117, mean_q: -17.226110\n",
      " 33411/50000: episode: 346, duration: 1.653s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 0.234532, mae: 14.363459, mean_q: -17.672371\n",
      " 33412/50000: episode: 347, duration: 1.638s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 0.229767, mae: 14.361893, mean_q: -17.673317\n",
      " 33561/50000: episode: 348, duration: 3.033s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.899 [0.000, 4.000], mean observation: 134.674 [0.000, 1000.000], loss: 2.622381, mae: 14.448354, mean_q: -17.625355\n",
      " 33710/50000: episode: 349, duration: 3.073s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.893 [0.000, 4.000], mean observation: 126.620 [0.000, 1000.000], loss: 2.886019, mae: 14.505066, mean_q: -17.710466\n",
      " 33736/50000: episode: 350, duration: 2.073s, episode steps: 26, steps per second: 13, episode reward: -24.000, mean reward: -0.923 [-1.000, 1.000], mean action: 1.538 [0.000, 4.000], mean observation: 129.738 [0.000, 1000.000], loss: 2.894047, mae: 14.532187, mean_q: -17.768215\n",
      " 33750/50000: episode: 351, duration: 1.968s, episode steps: 14, steps per second: 7, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 137.029 [0.000, 1000.000], loss: 2.434168, mae: 14.521220, mean_q: -17.765482\n",
      " 33754/50000: episode: 352, duration: 1.897s, episode steps: 4, steps per second: 2, episode reward: -2.000, mean reward: -0.500 [-1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 100.650 [0.000, 1000.000], loss: 2.751758, mae: 14.534004, mean_q: -17.837378\n",
      " 33853/50000: episode: 353, duration: 2.624s, episode steps: 99, steps per second: 38, episode reward: -97.000, mean reward: -0.980 [-1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 138.735 [0.000, 1000.000], loss: 2.139542, mae: 14.544725, mean_q: -17.787094\n",
      " 34002/50000: episode: 354, duration: 2.848s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.926 [0.000, 4.000], mean observation: 135.904 [0.000, 1000.000], loss: 2.226441, mae: 14.591352, mean_q: -17.878016\n",
      " 34075/50000: episode: 355, duration: 2.390s, episode steps: 73, steps per second: 31, episode reward: -71.000, mean reward: -0.973 [-1.000, 1.000], mean action: 1.973 [0.000, 4.000], mean observation: 131.652 [0.000, 1000.000], loss: 2.576170, mae: 14.649031, mean_q: -17.894602\n",
      " 34224/50000: episode: 356, duration: 2.844s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.074 [0.000, 4.000], mean observation: 135.801 [0.000, 1000.000], loss: 2.600450, mae: 14.689799, mean_q: -17.976835\n",
      "re_generate cnt=2\n",
      " 34298/50000: episode: 357, duration: 2.398s, episode steps: 74, steps per second: 31, episode reward: -72.000, mean reward: -0.973 [-1.000, 1.000], mean action: 1.851 [0.000, 4.000], mean observation: 148.466 [0.000, 1000.000], loss: 2.803667, mae: 14.736866, mean_q: -18.003124\n",
      "re_generate cnt=2\n",
      " 34325/50000: episode: 358, duration: 2.034s, episode steps: 27, steps per second: 13, episode reward: -25.000, mean reward: -0.926 [-1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 138.715 [0.000, 1000.000], loss: 3.379679, mae: 14.761365, mean_q: -18.020412\n",
      "re_generate cnt=2\n",
      " 34356/50000: episode: 359, duration: 2.059s, episode steps: 31, steps per second: 15, episode reward: -29.000, mean reward: -0.935 [-1.000, 1.000], mean action: 2.032 [0.000, 4.000], mean observation: 133.916 [0.000, 1000.000], loss: 3.446162, mae: 14.770345, mean_q: -17.989145\n",
      " 34505/50000: episode: 360, duration: 2.886s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.946 [0.000, 4.000], mean observation: 136.134 [0.000, 1000.000], loss: 2.931093, mae: 14.787924, mean_q: -18.046637\n",
      " 34518/50000: episode: 361, duration: 1.884s, episode steps: 13, steps per second: 7, episode reward: -11.000, mean reward: -0.846 [-1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 124.577 [0.000, 1000.000], loss: 2.263122, mae: 14.806030, mean_q: -18.033052\n",
      " 34587/50000: episode: 362, duration: 2.268s, episode steps: 69, steps per second: 30, episode reward: -67.000, mean reward: -0.971 [-1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 134.587 [0.000, 1000.000], loss: 2.865495, mae: 14.829743, mean_q: -18.076864\n",
      " 34736/50000: episode: 363, duration: 2.969s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.020 [0.000, 4.000], mean observation: 138.835 [0.000, 1000.000], loss: 2.980169, mae: 14.864358, mean_q: -18.131920\n",
      " 34755/50000: episode: 364, duration: 2.037s, episode steps: 19, steps per second: 9, episode reward: -17.000, mean reward: -0.895 [-1.000, 1.000], mean action: 2.368 [1.000, 4.000], mean observation: 117.616 [0.000, 1000.000], loss: 2.789840, mae: 14.891722, mean_q: -18.125439\n",
      " 34904/50000: episode: 365, duration: 2.957s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.785 [0.000, 4.000], mean observation: 136.773 [0.000, 1000.000], loss: 2.895042, mae: 14.912208, mean_q: -18.217745\n",
      " 34932/50000: episode: 366, duration: 2.057s, episode steps: 28, steps per second: 14, episode reward: -26.000, mean reward: -0.929 [-1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 116.582 [0.000, 1000.000], loss: 2.292488, mae: 14.922991, mean_q: -18.244152\n",
      " 34943/50000: episode: 367, duration: 1.917s, episode steps: 11, steps per second: 6, episode reward: -9.000, mean reward: -0.818 [-1.000, 1.000], mean action: 1.818 [0.000, 4.000], mean observation: 92.227 [0.000, 1000.000], loss: 4.335878, mae: 14.973423, mean_q: -18.224674\n",
      " 34955/50000: episode: 368, duration: 1.944s, episode steps: 12, steps per second: 6, episode reward: -10.000, mean reward: -0.833 [-1.000, 1.000], mean action: 1.417 [0.000, 4.000], mean observation: 101.475 [0.000, 1000.000], loss: 2.514200, mae: 14.945561, mean_q: -18.203680\n",
      " 34978/50000: episode: 369, duration: 1.995s, episode steps: 23, steps per second: 12, episode reward: -21.000, mean reward: -0.913 [-1.000, 1.000], mean action: 1.870 [0.000, 4.000], mean observation: 97.457 [0.000, 1000.000], loss: 2.644426, mae: 14.946013, mean_q: -18.276131\n",
      " 35057/50000: episode: 370, duration: 2.432s, episode steps: 79, steps per second: 32, episode reward: -77.000, mean reward: -0.975 [-1.000, 1.000], mean action: 1.899 [0.000, 4.000], mean observation: 114.082 [0.000, 1000.000], loss: 3.246852, mae: 14.973186, mean_q: -18.248589\n",
      " 35078/50000: episode: 371, duration: 1.991s, episode steps: 21, steps per second: 11, episode reward: -19.000, mean reward: -0.905 [-1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 91.890 [0.000, 1000.000], loss: 1.693663, mae: 14.957001, mean_q: -18.244789\n",
      " 35100/50000: episode: 372, duration: 2.065s, episode steps: 22, steps per second: 11, episode reward: -20.000, mean reward: -0.909 [-1.000, 1.000], mean action: 2.364 [0.000, 4.000], mean observation: 119.045 [0.000, 1000.000], loss: 2.546513, mae: 14.977020, mean_q: -18.152388\n",
      " 35200/50000: episode: 373, duration: 2.571s, episode steps: 100, steps per second: 39, episode reward: -98.000, mean reward: -0.980 [-1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 136.298 [0.000, 1000.000], loss: 2.841300, mae: 15.003493, mean_q: -18.233599\n",
      " 35243/50000: episode: 374, duration: 2.149s, episode steps: 43, steps per second: 20, episode reward: -41.000, mean reward: -0.953 [-1.000, 1.000], mean action: 1.860 [0.000, 4.000], mean observation: 133.777 [0.000, 1000.000], loss: 3.105758, mae: 15.027506, mean_q: -18.275469\n",
      " 35392/50000: episode: 375, duration: 2.849s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.738 [0.000, 4.000], mean observation: 135.315 [0.000, 1000.000], loss: 3.268598, mae: 15.061255, mean_q: -18.349903\n",
      " 35529/50000: episode: 376, duration: 2.819s, episode steps: 137, steps per second: 49, episode reward: -135.000, mean reward: -0.985 [-1.000, 1.000], mean action: 1.985 [0.000, 4.000], mean observation: 135.347 [0.000, 1000.000], loss: 2.915859, mae: 15.085576, mean_q: -18.355530\n",
      " 35678/50000: episode: 377, duration: 2.842s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.906 [0.000, 4.000], mean observation: 129.804 [0.000, 1000.000], loss: 2.655761, mae: 15.119162, mean_q: -18.396534\n",
      " 35711/50000: episode: 378, duration: 1.957s, episode steps: 33, steps per second: 17, episode reward: -31.000, mean reward: -0.939 [-1.000, 1.000], mean action: 1.727 [0.000, 4.000], mean observation: 125.970 [0.000, 1000.000], loss: 3.564197, mae: 15.167600, mean_q: -18.457788\n",
      " 35732/50000: episode: 379, duration: 1.925s, episode steps: 21, steps per second: 11, episode reward: -19.000, mean reward: -0.905 [-1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 115.976 [0.000, 1000.000], loss: 2.173877, mae: 15.141088, mean_q: -18.451294\n",
      " 35784/50000: episode: 380, duration: 2.082s, episode steps: 52, steps per second: 25, episode reward: -50.000, mean reward: -0.962 [-1.000, 1.000], mean action: 2.096 [0.000, 4.000], mean observation: 127.333 [0.000, 1000.000], loss: 2.572011, mae: 15.161745, mean_q: -18.473866\n",
      "re_generate cnt=2\n",
      "re_generate cnt=3\n",
      "re_generate cnt=4\n",
      "re_generate cnt=5\n",
      "re_generate cnt=6\n",
      " 35794/50000: episode: 381, duration: 1.917s, episode steps: 10, steps per second: 5, episode reward: -8.000, mean reward: -0.800 [-1.000, 1.000], mean action: 2.200 [1.000, 4.000], mean observation: 111.130 [0.000, 1000.000], loss: 3.482020, mae: 15.184881, mean_q: -18.512119\n",
      "re_generate cnt=2\n",
      "re_generate cnt=3\n",
      "re_generate cnt=4\n",
      "re_generate cnt=5\n",
      "re_generate cnt=6\n",
      " 35822/50000: episode: 382, duration: 2.073s, episode steps: 28, steps per second: 14, episode reward: -26.000, mean reward: -0.929 [-1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 119.607 [0.000, 1000.000], loss: 3.006942, mae: 15.177365, mean_q: -18.504837\n",
      "re_generate cnt=2\n",
      "re_generate cnt=3\n",
      "re_generate cnt=4\n",
      "re_generate cnt=5\n",
      "re_generate cnt=6\n",
      " 35831/50000: episode: 383, duration: 1.992s, episode steps: 9, steps per second: 5, episode reward: -7.000, mean reward: -0.778 [-1.000, 1.000], mean action: 2.222 [0.000, 3.000], mean observation: 100.956 [0.000, 1000.000], loss: 2.927089, mae: 15.177358, mean_q: -18.453510\n",
      " 35912/50000: episode: 384, duration: 2.448s, episode steps: 81, steps per second: 33, episode reward: -79.000, mean reward: -0.975 [-1.000, 1.000], mean action: 1.951 [0.000, 4.000], mean observation: 141.464 [0.000, 1000.000], loss: 2.571556, mae: 15.192770, mean_q: -18.485409\n",
      " 35988/50000: episode: 385, duration: 2.405s, episode steps: 76, steps per second: 32, episode reward: -74.000, mean reward: -0.974 [-1.000, 1.000], mean action: 1.763 [0.000, 4.000], mean observation: 137.346 [0.000, 1000.000], loss: 3.111776, mae: 15.221556, mean_q: -18.495781\n",
      " 36096/50000: episode: 386, duration: 2.675s, episode steps: 108, steps per second: 40, episode reward: -106.000, mean reward: -0.981 [-1.000, 1.000], mean action: 1.972 [0.000, 4.000], mean observation: 138.056 [0.000, 1000.000], loss: 2.449862, mae: 15.229463, mean_q: -18.513777\n",
      " 36142/50000: episode: 387, duration: 2.256s, episode steps: 46, steps per second: 20, episode reward: -44.000, mean reward: -0.957 [-1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 134.661 [0.000, 1000.000], loss: 2.564959, mae: 15.252000, mean_q: -18.629166\n",
      " 36156/50000: episode: 388, duration: 1.935s, episode steps: 14, steps per second: 7, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 122.750 [0.000, 1000.000], loss: 2.894215, mae: 15.264330, mean_q: -18.657595\n",
      " 36204/50000: episode: 389, duration: 2.178s, episode steps: 48, steps per second: 22, episode reward: -46.000, mean reward: -0.958 [-1.000, 1.000], mean action: 1.917 [0.000, 4.000], mean observation: 139.485 [0.000, 1000.000], loss: 3.814983, mae: 15.294211, mean_q: -18.541468\n",
      " 36209/50000: episode: 390, duration: 1.767s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 3.200 [3.000, 4.000], mean observation: 0.400 [0.000, 4.000], loss: 6.267223, mae: 15.368273, mean_q: -18.466339\n",
      " 36214/50000: episode: 391, duration: 1.765s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.400 [0.000, 4.000], loss: 3.514452, mae: 15.288257, mean_q: -18.668690\n",
      " 36219/50000: episode: 392, duration: 1.854s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 2.200 [1.000, 4.000], mean observation: 0.400 [0.000, 4.000], loss: 4.125414, mae: 15.295199, mean_q: -18.568369\n",
      " 36368/50000: episode: 393, duration: 2.975s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 128.846 [0.000, 1000.000], loss: 2.920307, mae: 15.303275, mean_q: -18.614153\n",
      " 36517/50000: episode: 394, duration: 2.976s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.094 [0.000, 4.000], mean observation: 137.071 [0.000, 1000.000], loss: 3.294895, mae: 15.344916, mean_q: -18.654572\n",
      " 36625/50000: episode: 395, duration: 2.741s, episode steps: 108, steps per second: 39, episode reward: -106.000, mean reward: -0.981 [-1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 113.619 [0.000, 1000.000], loss: 3.895231, mae: 15.379971, mean_q: -18.659956\n",
      " 36774/50000: episode: 396, duration: 3.005s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.020 [0.000, 4.000], mean observation: 136.872 [0.000, 1000.000], loss: 3.617084, mae: 15.396845, mean_q: -18.686911\n",
      " 36923/50000: episode: 397, duration: 3.012s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.054 [0.000, 4.000], mean observation: 132.775 [0.000, 1000.000], loss: 3.358302, mae: 15.423235, mean_q: -18.745867\n",
      " 37072/50000: episode: 398, duration: 3.008s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.000 [0.000, 4.000], mean observation: 132.039 [0.000, 1000.000], loss: 2.578169, mae: 15.429012, mean_q: -18.775881\n",
      " 37196/50000: episode: 399, duration: 2.762s, episode steps: 124, steps per second: 45, episode reward: -122.000, mean reward: -0.984 [-1.000, 1.000], mean action: 2.137 [0.000, 4.000], mean observation: 138.580 [0.000, 1000.000], loss: 2.909369, mae: 15.467333, mean_q: -18.796167\n",
      " 37345/50000: episode: 400, duration: 2.963s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.067 [0.000, 4.000], mean observation: 148.132 [0.000, 1000.000], loss: 2.780896, mae: 15.491998, mean_q: -18.848568\n",
      " 37494/50000: episode: 401, duration: 2.955s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.329 [0.000, 4.000], mean observation: 149.940 [0.000, 1000.000], loss: 2.714362, mae: 15.531322, mean_q: -18.893902\n",
      " 37643/50000: episode: 402, duration: 3.096s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.107 [0.000, 4.000], mean observation: 139.833 [0.000, 1000.000], loss: 2.917951, mae: 15.571522, mean_q: -18.984812\n",
      " 37645/50000: episode: 403, duration: 1.861s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 50.600 [0.000, 1000.000], loss: 6.552989, mae: 15.657049, mean_q: -19.075617\n",
      " 37647/50000: episode: 404, duration: 1.868s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 50.600 [0.000, 1000.000], loss: 0.185891, mae: 15.528554, mean_q: -19.076813\n",
      " 37739/50000: episode: 405, duration: 2.512s, episode steps: 92, steps per second: 37, episode reward: -90.000, mean reward: -0.978 [-1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 137.687 [0.000, 1000.000], loss: 2.569845, mae: 15.596527, mean_q: -19.006800\n",
      " 37778/50000: episode: 406, duration: 2.166s, episode steps: 39, steps per second: 18, episode reward: -37.000, mean reward: -0.949 [-1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 137.490 [0.000, 1000.000], loss: 2.078733, mae: 15.609102, mean_q: -19.071354\n",
      " 37809/50000: episode: 407, duration: 2.063s, episode steps: 31, steps per second: 15, episode reward: -29.000, mean reward: -0.935 [-1.000, 1.000], mean action: 1.871 [0.000, 4.000], mean observation: 133.752 [0.000, 1000.000], loss: 2.240027, mae: 15.624894, mean_q: -19.062954\n",
      " 37932/50000: episode: 408, duration: 2.815s, episode steps: 123, steps per second: 44, episode reward: -121.000, mean reward: -0.984 [-1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 121.828 [0.000, 1000.000], loss: 2.921722, mae: 15.657059, mean_q: -19.130745\n",
      " 38081/50000: episode: 409, duration: 3.045s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.866 [0.000, 4.000], mean observation: 126.652 [0.000, 1000.000], loss: 2.895763, mae: 15.694713, mean_q: -19.165146\n",
      " 38180/50000: episode: 410, duration: 2.687s, episode steps: 99, steps per second: 37, episode reward: -97.000, mean reward: -0.980 [-1.000, 1.000], mean action: 1.980 [0.000, 4.000], mean observation: 134.207 [0.000, 1000.000], loss: 2.494408, mae: 15.723761, mean_q: -19.226084\n",
      " 38329/50000: episode: 411, duration: 3.171s, episode steps: 149, steps per second: 47, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.859 [0.000, 4.000], mean observation: 144.115 [0.000, 1000.000], loss: 2.800565, mae: 15.768732, mean_q: -19.266504\n",
      " 38359/50000: episode: 412, duration: 2.099s, episode steps: 30, steps per second: 14, episode reward: -28.000, mean reward: -0.933 [-1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 116.343 [0.000, 1000.000], loss: 3.135998, mae: 15.803640, mean_q: -19.289520\n",
      " 38508/50000: episode: 413, duration: 3.059s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [0.000, 4.000], mean observation: 137.818 [0.000, 1000.000], loss: 2.097403, mae: 15.810121, mean_q: -19.361399\n",
      " 38538/50000: episode: 414, duration: 2.042s, episode steps: 30, steps per second: 15, episode reward: -28.000, mean reward: -0.933 [-1.000, 1.000], mean action: 2.033 [0.000, 4.000], mean observation: 125.287 [0.000, 1000.000], loss: 3.358152, mae: 15.863633, mean_q: -19.421427\n",
      " 38640/50000: episode: 415, duration: 2.595s, episode steps: 102, steps per second: 39, episode reward: -100.000, mean reward: -0.980 [-1.000, 1.000], mean action: 1.990 [0.000, 4.000], mean observation: 140.694 [0.000, 1000.000], loss: 3.097874, mae: 15.884292, mean_q: -19.406996\n",
      " 38789/50000: episode: 416, duration: 2.958s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.832 [0.000, 4.000], mean observation: 142.083 [0.000, 1000.000], loss: 2.639011, mae: 15.909193, mean_q: -19.465126\n",
      " 38938/50000: episode: 417, duration: 2.942s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.993 [0.000, 4.000], mean observation: 149.541 [0.000, 1000.000], loss: 2.688214, mae: 15.958719, mean_q: -19.566175\n",
      " 39012/50000: episode: 418, duration: 2.385s, episode steps: 74, steps per second: 31, episode reward: -72.000, mean reward: -0.973 [-1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 144.901 [0.000, 1000.000], loss: 2.124025, mae: 15.990274, mean_q: -19.640717\n",
      " 39161/50000: episode: 419, duration: 2.944s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.933 [0.000, 4.000], mean observation: 153.781 [0.000, 1000.000], loss: 2.673576, mae: 16.040556, mean_q: -19.664373\n",
      " 39171/50000: episode: 420, duration: 1.985s, episode steps: 10, steps per second: 5, episode reward: -8.000, mean reward: -0.800 [-1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 90.770 [0.000, 1000.000], loss: 3.313845, mae: 16.075506, mean_q: -19.696054\n",
      " 39182/50000: episode: 421, duration: 1.962s, episode steps: 11, steps per second: 6, episode reward: -9.000, mean reward: -0.818 [-1.000, 1.000], mean action: 2.727 [1.000, 4.000], mean observation: 119.191 [0.000, 1000.000], loss: 1.275491, mae: 16.044596, mean_q: -19.661644\n",
      " 39198/50000: episode: 422, duration: 1.947s, episode steps: 16, steps per second: 8, episode reward: -14.000, mean reward: -0.875 [-1.000, 1.000], mean action: 1.438 [0.000, 4.000], mean observation: 144.844 [0.000, 1000.000], loss: 3.267982, mae: 16.091335, mean_q: -19.733955\n",
      " 39347/50000: episode: 423, duration: 2.976s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 131.762 [0.000, 1000.000], loss: 2.565127, mae: 16.099934, mean_q: -19.738148\n",
      " 39430/50000: episode: 424, duration: 2.481s, episode steps: 83, steps per second: 33, episode reward: -81.000, mean reward: -0.976 [-1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 127.240 [0.000, 1000.000], loss: 2.117332, mae: 16.139460, mean_q: -19.836634\n",
      " 39518/50000: episode: 425, duration: 2.569s, episode steps: 88, steps per second: 34, episode reward: -86.000, mean reward: -0.977 [-1.000, 1.000], mean action: 2.148 [0.000, 4.000], mean observation: 123.492 [0.000, 1000.000], loss: 3.041575, mae: 16.187080, mean_q: -19.856615\n",
      " 39667/50000: episode: 426, duration: 2.978s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.846 [0.000, 4.000], mean observation: 122.393 [0.000, 1000.000], loss: 2.971985, mae: 16.224657, mean_q: -19.899027\n",
      " 39816/50000: episode: 427, duration: 2.977s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.933 [0.000, 4.000], mean observation: 122.753 [0.000, 1000.000], loss: 3.256768, mae: 16.278017, mean_q: -19.961161\n",
      " 39878/50000: episode: 428, duration: 2.299s, episode steps: 62, steps per second: 27, episode reward: -60.000, mean reward: -0.968 [-1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 118.355 [0.000, 1000.000], loss: 2.477188, mae: 16.299030, mean_q: -20.013906\n",
      " 40027/50000: episode: 429, duration: 2.916s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.846 [0.000, 4.000], mean observation: 138.007 [0.000, 1000.000], loss: 2.642282, mae: 16.337862, mean_q: -20.064854\n",
      " 40052/50000: episode: 430, duration: 1.997s, episode steps: 25, steps per second: 13, episode reward: -23.000, mean reward: -0.920 [-1.000, 1.000], mean action: 1.720 [0.000, 4.000], mean observation: 133.784 [0.000, 1000.000], loss: 3.025013, mae: 16.371592, mean_q: -20.087465\n",
      " 40201/50000: episode: 431, duration: 2.908s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [0.000, 4.000], mean observation: 140.713 [0.000, 1000.000], loss: 3.056515, mae: 16.405848, mean_q: -20.123228\n",
      " 40222/50000: episode: 432, duration: 1.990s, episode steps: 21, steps per second: 11, episode reward: -19.000, mean reward: -0.905 [-1.000, 1.000], mean action: 1.571 [0.000, 4.000], mean observation: 129.948 [0.000, 1000.000], loss: 3.162299, mae: 16.434795, mean_q: -20.183109\n",
      " 40351/50000: episode: 433, duration: 2.908s, episode steps: 129, steps per second: 44, episode reward: -127.000, mean reward: -0.984 [-1.000, 1.000], mean action: 1.860 [0.000, 4.000], mean observation: 140.409 [0.000, 1000.000], loss: 2.232659, mae: 16.446283, mean_q: -20.198099\n",
      " 40370/50000: episode: 434, duration: 1.970s, episode steps: 19, steps per second: 10, episode reward: -17.000, mean reward: -0.895 [-1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 122.405 [0.000, 1000.000], loss: 4.058860, mae: 16.514368, mean_q: -20.296431\n",
      " 40380/50000: episode: 435, duration: 1.869s, episode steps: 10, steps per second: 5, episode reward: -8.000, mean reward: -0.800 [-1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 131.100 [0.000, 1000.000], loss: 3.038265, mae: 16.501240, mean_q: -20.203457\n",
      " 40422/50000: episode: 436, duration: 2.119s, episode steps: 42, steps per second: 20, episode reward: -40.000, mean reward: -0.952 [-1.000, 1.000], mean action: 1.429 [0.000, 4.000], mean observation: 130.448 [0.000, 1000.000], loss: 3.615713, mae: 16.519360, mean_q: -20.248348\n",
      " 40513/50000: episode: 437, duration: 2.622s, episode steps: 91, steps per second: 35, episode reward: -89.000, mean reward: -0.978 [-1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 137.534 [0.000, 1000.000], loss: 3.080121, mae: 16.533178, mean_q: -20.319895\n",
      " 40643/50000: episode: 438, duration: 2.918s, episode steps: 130, steps per second: 45, episode reward: -128.000, mean reward: -0.985 [-1.000, 1.000], mean action: 1.885 [0.000, 4.000], mean observation: 129.994 [0.000, 1000.000], loss: 2.811692, mae: 16.561501, mean_q: -20.333515\n",
      " 40792/50000: episode: 439, duration: 2.985s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.819 [0.000, 4.000], mean observation: 129.426 [0.000, 1000.000], loss: 2.961381, mae: 16.613619, mean_q: -20.408482\n",
      " 40829/50000: episode: 440, duration: 2.099s, episode steps: 37, steps per second: 18, episode reward: -35.000, mean reward: -0.946 [-1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 124.508 [0.000, 1000.000], loss: 2.769222, mae: 16.643127, mean_q: -20.438789\n",
      " 40891/50000: episode: 441, duration: 2.439s, episode steps: 62, steps per second: 25, episode reward: -60.000, mean reward: -0.968 [-1.000, 1.000], mean action: 1.758 [0.000, 4.000], mean observation: 142.179 [0.000, 1000.000], loss: 2.948289, mae: 16.663656, mean_q: -20.487446\n",
      " 41040/50000: episode: 442, duration: 3.115s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.020 [0.000, 4.000], mean observation: 130.250 [0.000, 1000.000], loss: 3.007829, mae: 16.701260, mean_q: -20.532341\n",
      " 41087/50000: episode: 443, duration: 2.302s, episode steps: 47, steps per second: 20, episode reward: -45.000, mean reward: -0.957 [-1.000, 1.000], mean action: 1.468 [0.000, 4.000], mean observation: 138.670 [0.000, 1000.000], loss: 3.081157, mae: 16.734968, mean_q: -20.570490\n",
      " 41236/50000: episode: 444, duration: 3.209s, episode steps: 149, steps per second: 46, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.940 [0.000, 4.000], mean observation: 141.674 [0.000, 1000.000], loss: 2.750701, mae: 16.762737, mean_q: -20.635515\n",
      " 41255/50000: episode: 445, duration: 2.177s, episode steps: 19, steps per second: 9, episode reward: -17.000, mean reward: -0.895 [-1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 133.253 [0.000, 1000.000], loss: 1.731886, mae: 16.766708, mean_q: -20.705017\n",
      " 41404/50000: episode: 446, duration: 3.194s, episode steps: 149, steps per second: 47, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.933 [0.000, 4.000], mean observation: 156.470 [0.000, 1000.000], loss: 2.319090, mae: 16.814842, mean_q: -20.704865\n",
      " 41553/50000: episode: 447, duration: 2.838s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.866 [0.000, 4.000], mean observation: 138.550 [0.000, 1000.000], loss: 2.065183, mae: 16.866440, mean_q: -20.750904\n",
      " 41702/50000: episode: 448, duration: 2.941s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.906 [0.000, 4.000], mean observation: 134.256 [0.000, 1000.000], loss: 2.309821, mae: 16.929344, mean_q: -20.854168\n",
      " 41851/50000: episode: 449, duration: 2.831s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.933 [0.000, 4.000], mean observation: 136.741 [0.000, 1000.000], loss: 2.253717, mae: 16.987783, mean_q: -20.907335\n",
      " 41997/50000: episode: 450, duration: 2.945s, episode steps: 146, steps per second: 50, episode reward: -144.000, mean reward: -0.986 [-1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 140.404 [0.000, 1000.000], loss: 2.475819, mae: 17.052454, mean_q: -20.972145\n",
      " 42035/50000: episode: 451, duration: 2.127s, episode steps: 38, steps per second: 18, episode reward: -36.000, mean reward: -0.947 [-1.000, 1.000], mean action: 2.211 [0.000, 4.000], mean observation: 149.516 [0.000, 1000.000], loss: 2.035943, mae: 17.076685, mean_q: -21.007961\n",
      " 42184/50000: episode: 452, duration: 3.002s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.913 [0.000, 4.000], mean observation: 139.836 [0.000, 1000.000], loss: 2.939201, mae: 17.129189, mean_q: -21.025959\n",
      " 42333/50000: episode: 453, duration: 2.804s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.148 [0.000, 4.000], mean observation: 143.763 [0.000, 1000.000], loss: 2.670902, mae: 17.176554, mean_q: -21.133347\n",
      " 42400/50000: episode: 454, duration: 2.146s, episode steps: 67, steps per second: 31, episode reward: -65.000, mean reward: -0.970 [-1.000, 1.000], mean action: 1.776 [0.000, 4.000], mean observation: 135.361 [0.000, 1000.000], loss: 2.880910, mae: 17.219255, mean_q: -21.162214\n",
      " 42488/50000: episode: 455, duration: 2.310s, episode steps: 88, steps per second: 38, episode reward: -86.000, mean reward: -0.977 [-1.000, 1.000], mean action: 1.898 [0.000, 4.000], mean observation: 128.972 [0.000, 1000.000], loss: 2.553394, mae: 17.241486, mean_q: -21.178778\n",
      " 42505/50000: episode: 456, duration: 2.004s, episode steps: 17, steps per second: 8, episode reward: -15.000, mean reward: -0.882 [-1.000, 1.000], mean action: 1.765 [0.000, 4.000], mean observation: 107.535 [0.000, 1000.000], loss: 2.163839, mae: 17.251509, mean_q: -21.210129\n",
      " 42518/50000: episode: 457, duration: 1.895s, episode steps: 13, steps per second: 7, episode reward: -11.000, mean reward: -0.846 [-1.000, 1.000], mean action: 2.231 [0.000, 4.000], mean observation: 78.285 [0.000, 1000.000], loss: 3.637285, mae: 17.281212, mean_q: -21.151583\n",
      " 42598/50000: episode: 458, duration: 2.548s, episode steps: 80, steps per second: 31, episode reward: -78.000, mean reward: -0.975 [-1.000, 1.000], mean action: 2.075 [0.000, 4.000], mean observation: 114.806 [0.000, 1000.000], loss: 3.525137, mae: 17.297436, mean_q: -21.204874\n",
      " 42739/50000: episode: 459, duration: 2.865s, episode steps: 141, steps per second: 49, episode reward: -139.000, mean reward: -0.986 [-1.000, 1.000], mean action: 1.723 [0.000, 4.000], mean observation: 133.991 [0.000, 1000.000], loss: 2.903346, mae: 17.318647, mean_q: -21.273180\n",
      " 42888/50000: episode: 460, duration: 2.902s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.893 [0.000, 4.000], mean observation: 141.491 [0.000, 1000.000], loss: 2.325328, mae: 17.357443, mean_q: -21.346209\n",
      " 42917/50000: episode: 461, duration: 1.887s, episode steps: 29, steps per second: 15, episode reward: -27.000, mean reward: -0.931 [-1.000, 1.000], mean action: 1.724 [0.000, 4.000], mean observation: 129.479 [0.000, 1000.000], loss: 2.529505, mae: 17.389254, mean_q: -21.411613\n",
      " 43066/50000: episode: 462, duration: 3.024s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.893 [0.000, 4.000], mean observation: 137.038 [0.000, 1000.000], loss: 2.372995, mae: 17.420794, mean_q: -21.412437\n",
      " 43215/50000: episode: 463, duration: 3.014s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.114 [0.000, 4.000], mean observation: 135.009 [0.000, 1000.000], loss: 2.586137, mae: 17.473883, mean_q: -21.470306\n",
      " 43364/50000: episode: 464, duration: 3.017s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.758 [0.000, 4.000], mean observation: 133.641 [0.000, 1000.000], loss: 2.246915, mae: 17.518211, mean_q: -21.536928\n",
      " 43513/50000: episode: 465, duration: 2.878s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.765 [0.000, 4.000], mean observation: 118.236 [0.000, 1000.000], loss: 2.862575, mae: 17.582123, mean_q: -21.613728\n",
      " 43566/50000: episode: 466, duration: 2.109s, episode steps: 53, steps per second: 25, episode reward: -51.000, mean reward: -0.962 [-1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 116.542 [0.000, 1000.000], loss: 1.783439, mae: 17.592682, mean_q: -21.655087\n",
      " 43715/50000: episode: 467, duration: 2.872s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.805 [0.000, 4.000], mean observation: 114.911 [0.000, 1000.000], loss: 2.986516, mae: 17.649689, mean_q: -21.670658\n",
      " 43864/50000: episode: 468, duration: 3.214s, episode steps: 149, steps per second: 46, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.034 [0.000, 4.000], mean observation: 132.581 [0.000, 1000.000], loss: 2.808053, mae: 17.688992, mean_q: -21.713873\n",
      " 43932/50000: episode: 469, duration: 2.440s, episode steps: 68, steps per second: 28, episode reward: -66.000, mean reward: -0.971 [-1.000, 1.000], mean action: 2.044 [0.000, 4.000], mean observation: 136.269 [0.000, 1000.000], loss: 2.369525, mae: 17.711334, mean_q: -21.754669\n",
      " 44081/50000: episode: 470, duration: 3.215s, episode steps: 149, steps per second: 46, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.000 [0.000, 4.000], mean observation: 130.590 [0.000, 1000.000], loss: 2.630081, mae: 17.750565, mean_q: -21.791704\n",
      " 44230/50000: episode: 471, duration: 3.308s, episode steps: 149, steps per second: 45, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.839 [0.000, 4.000], mean observation: 137.436 [0.000, 1000.000], loss: 2.739613, mae: 17.791756, mean_q: -21.838602\n",
      " 44379/50000: episode: 472, duration: 3.262s, episode steps: 149, steps per second: 46, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.886 [0.000, 4.000], mean observation: 134.179 [0.000, 1000.000], loss: 2.206300, mae: 17.827793, mean_q: -21.882372\n",
      " 44485/50000: episode: 473, duration: 2.672s, episode steps: 106, steps per second: 40, episode reward: -104.000, mean reward: -0.981 [-1.000, 1.000], mean action: 1.981 [0.000, 4.000], mean observation: 138.700 [0.000, 1000.000], loss: 2.074607, mae: 17.866144, mean_q: -21.943911\n",
      " 44634/50000: episode: 474, duration: 3.014s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.906 [0.000, 4.000], mean observation: 135.262 [0.000, 1000.000], loss: 2.566042, mae: 17.912561, mean_q: -21.997553\n",
      " 44783/50000: episode: 475, duration: 2.999s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.893 [0.000, 4.000], mean observation: 139.649 [0.000, 1000.000], loss: 1.885192, mae: 17.945860, mean_q: -22.049137\n",
      " 44787/50000: episode: 476, duration: 1.878s, episode steps: 4, steps per second: 2, episode reward: -2.000, mean reward: -0.500 [-1.000, 1.000], mean action: 2.500 [1.000, 3.000], mean observation: 50.725 [0.000, 1000.000], loss: 2.271809, mae: 17.981567, mean_q: -22.097528\n",
      " 44936/50000: episode: 477, duration: 2.825s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.859 [0.000, 4.000], mean observation: 146.041 [0.000, 1000.000], loss: 2.063284, mae: 17.999693, mean_q: -22.110493\n",
      " 45085/50000: episode: 478, duration: 2.835s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.745 [0.000, 4.000], mean observation: 137.970 [0.000, 1000.000], loss: 2.036510, mae: 18.045473, mean_q: -22.149935\n",
      " 45185/50000: episode: 479, duration: 2.436s, episode steps: 100, steps per second: 41, episode reward: -98.000, mean reward: -0.980 [-1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 127.768 [0.000, 1000.000], loss: 1.719151, mae: 18.079226, mean_q: -22.210381\n",
      " 45199/50000: episode: 480, duration: 1.927s, episode steps: 14, steps per second: 7, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 2.571 [1.000, 4.000], mean observation: 108.729 [0.000, 1000.000], loss: 1.161881, mae: 18.086102, mean_q: -22.240690\n",
      " 45229/50000: episode: 481, duration: 2.056s, episode steps: 30, steps per second: 15, episode reward: -28.000, mean reward: -0.933 [-1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 112.303 [0.000, 1000.000], loss: 3.164603, mae: 18.132763, mean_q: -22.226900\n",
      " 45243/50000: episode: 482, duration: 1.950s, episode steps: 14, steps per second: 7, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 108.729 [0.000, 1000.000], loss: 3.990405, mae: 18.155870, mean_q: -22.139952\n",
      " 45348/50000: episode: 483, duration: 2.609s, episode steps: 105, steps per second: 40, episode reward: -103.000, mean reward: -0.981 [-1.000, 1.000], mean action: 2.238 [0.000, 4.000], mean observation: 130.350 [0.000, 1000.000], loss: 2.835180, mae: 18.147812, mean_q: -22.230520\n",
      " 45433/50000: episode: 484, duration: 2.467s, episode steps: 85, steps per second: 34, episode reward: -83.000, mean reward: -0.976 [-1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 136.431 [0.000, 1000.000], loss: 1.859315, mae: 18.156984, mean_q: -22.269785\n",
      " 45453/50000: episode: 485, duration: 1.978s, episode steps: 20, steps per second: 10, episode reward: -18.000, mean reward: -0.900 [-1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 136.790 [0.000, 1000.000], loss: 2.444647, mae: 18.180996, mean_q: -22.234192\n",
      " 45456/50000: episode: 486, duration: 1.926s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 1.000 [0.000, 2.000], mean observation: 0.200 [0.000, 2.000], loss: 0.152748, mae: 18.143946, mean_q: -22.304476\n",
      " 45459/50000: episode: 487, duration: 2.014s, episode steps: 3, steps per second: 1, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.200 [0.000, 2.000], loss: 1.108382, mae: 18.169617, mean_q: -22.304907\n",
      " 45462/50000: episode: 488, duration: 1.827s, episode steps: 3, steps per second: 2, episode reward: -1.000, mean reward: -0.333 [-1.000, 1.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.200 [0.000, 2.000], loss: 0.485917, mae: 18.171183, mean_q: -22.148949\n",
      " 45528/50000: episode: 489, duration: 2.434s, episode steps: 66, steps per second: 27, episode reward: -64.000, mean reward: -0.970 [-1.000, 1.000], mean action: 1.970 [0.000, 4.000], mean observation: 142.221 [0.000, 1000.000], loss: 2.919036, mae: 18.210140, mean_q: -22.282475\n",
      " 45569/50000: episode: 490, duration: 2.277s, episode steps: 41, steps per second: 18, episode reward: -39.000, mean reward: -0.951 [-1.000, 1.000], mean action: 2.244 [0.000, 4.000], mean observation: 138.339 [0.000, 1000.000], loss: 2.486614, mae: 18.213537, mean_q: -22.291670\n",
      " 45704/50000: episode: 491, duration: 2.907s, episode steps: 135, steps per second: 46, episode reward: -133.000, mean reward: -0.985 [-1.000, 1.000], mean action: 1.778 [0.000, 4.000], mean observation: 143.304 [0.000, 1000.000], loss: 2.357397, mae: 18.234688, mean_q: -22.346813\n",
      " 45853/50000: episode: 492, duration: 3.233s, episode steps: 149, steps per second: 46, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.926 [0.000, 4.000], mean observation: 129.467 [0.000, 1000.000], loss: 2.353362, mae: 18.273729, mean_q: -22.395817\n",
      " 46002/50000: episode: 493, duration: 3.027s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.027 [0.000, 4.000], mean observation: 132.585 [0.000, 1000.000], loss: 3.075306, mae: 18.322309, mean_q: -22.440283\n",
      " 46151/50000: episode: 494, duration: 3.013s, episode steps: 149, steps per second: 49, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.054 [0.000, 4.000], mean observation: 135.528 [0.000, 1000.000], loss: 2.883904, mae: 18.353643, mean_q: -22.492319\n",
      " 46300/50000: episode: 495, duration: 3.085s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.268 [0.000, 4.000], mean observation: 154.134 [0.000, 1000.000], loss: 2.821582, mae: 18.391987, mean_q: -22.550497\n",
      " 46449/50000: episode: 496, duration: 3.128s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 144.138 [0.000, 1000.000], loss: 3.083856, mae: 18.438030, mean_q: -22.598019\n",
      " 46598/50000: episode: 497, duration: 2.974s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 143.376 [0.000, 1000.000], loss: 2.921880, mae: 18.472160, mean_q: -22.673124\n",
      " 46599/50000: episode: 498, duration: 1.804s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 0.169035, mae: 18.443211, mean_q: -22.713074\n",
      " 46600/50000: episode: 499, duration: 1.810s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 1.879390, mae: 18.494442, mean_q: -22.407894\n",
      " 46601/50000: episode: 500, duration: 1.820s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.000 [0.000, 0.000], loss: 7.691958, mae: 18.577076, mean_q: -22.714958\n",
      " 46718/50000: episode: 501, duration: 2.671s, episode steps: 117, steps per second: 44, episode reward: -115.000, mean reward: -0.983 [-1.000, 1.000], mean action: 1.949 [0.000, 4.000], mean observation: 127.650 [0.000, 1000.000], loss: 2.448021, mae: 18.504799, mean_q: -22.727428\n",
      " 46784/50000: episode: 502, duration: 2.298s, episode steps: 66, steps per second: 29, episode reward: -64.000, mean reward: -0.970 [-1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 126.055 [0.000, 1000.000], loss: 2.611446, mae: 18.533073, mean_q: -22.775093\n",
      " 46933/50000: episode: 503, duration: 2.929s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.946 [0.000, 4.000], mean observation: 126.783 [0.000, 1000.000], loss: 3.115319, mae: 18.573170, mean_q: -22.816954\n",
      " 47082/50000: episode: 504, duration: 3.079s, episode steps: 149, steps per second: 48, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.040 [0.000, 4.000], mean observation: 128.760 [0.000, 1000.000], loss: 2.352443, mae: 18.606213, mean_q: -22.866251\n",
      " 47231/50000: episode: 505, duration: 2.976s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.966 [0.000, 4.000], mean observation: 125.913 [0.000, 1000.000], loss: 2.820165, mae: 18.662102, mean_q: -22.914278\n",
      " 47380/50000: episode: 506, duration: 3.002s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 136.257 [0.000, 1000.000], loss: 3.253387, mae: 18.708902, mean_q: -22.952404\n",
      " 47402/50000: episode: 507, duration: 1.862s, episode steps: 22, steps per second: 12, episode reward: -20.000, mean reward: -0.909 [-1.000, 1.000], mean action: 1.773 [0.000, 4.000], mean observation: 96.650 [0.000, 1000.000], loss: 2.096370, mae: 18.717028, mean_q: -22.981499\n",
      " 47416/50000: episode: 508, duration: 1.821s, episode steps: 14, steps per second: 8, episode reward: -12.000, mean reward: -0.857 [-1.000, 1.000], mean action: 2.357 [0.000, 4.000], mean observation: 86.786 [0.000, 1000.000], loss: 3.248340, mae: 18.738924, mean_q: -22.975887\n",
      " 47453/50000: episode: 509, duration: 2.057s, episode steps: 37, steps per second: 18, episode reward: -35.000, mean reward: -0.946 [-1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 117.689 [0.000, 1000.000], loss: 1.900591, mae: 18.718250, mean_q: -22.969315\n",
      " 47602/50000: episode: 510, duration: 2.980s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.094 [0.000, 4.000], mean observation: 123.638 [0.000, 1000.000], loss: 3.044099, mae: 18.760439, mean_q: -23.011850\n",
      " 47693/50000: episode: 511, duration: 2.646s, episode steps: 91, steps per second: 34, episode reward: -89.000, mean reward: -0.978 [-1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 126.120 [0.000, 1000.000], loss: 2.836728, mae: 18.788830, mean_q: -23.062435\n",
      " 47720/50000: episode: 512, duration: 2.003s, episode steps: 27, steps per second: 13, episode reward: -25.000, mean reward: -0.926 [-1.000, 1.000], mean action: 2.074 [0.000, 4.000], mean observation: 124.548 [0.000, 1000.000], loss: 2.049321, mae: 18.789513, mean_q: -23.055332\n",
      " 47869/50000: episode: 513, duration: 2.846s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.020 [0.000, 4.000], mean observation: 143.501 [0.000, 1000.000], loss: 2.969125, mae: 18.829098, mean_q: -23.099865\n",
      " 47919/50000: episode: 514, duration: 2.153s, episode steps: 50, steps per second: 23, episode reward: -48.000, mean reward: -0.960 [-1.000, 1.000], mean action: 2.280 [0.000, 4.000], mean observation: 143.990 [0.000, 1000.000], loss: 3.378997, mae: 18.867682, mean_q: -23.168283\n",
      " 48068/50000: episode: 515, duration: 2.840s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.993 [0.000, 4.000], mean observation: 145.455 [0.000, 1000.000], loss: 2.855489, mae: 18.884369, mean_q: -23.215435\n",
      " 48217/50000: episode: 516, duration: 2.793s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.054 [0.000, 4.000], mean observation: 151.775 [0.000, 1000.000], loss: 2.953228, mae: 18.926270, mean_q: -23.274658\n",
      " 48366/50000: episode: 517, duration: 2.854s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.027 [0.000, 4.000], mean observation: 148.152 [0.000, 1000.000], loss: 3.209616, mae: 18.970013, mean_q: -23.321085\n",
      " 48515/50000: episode: 518, duration: 2.804s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.926 [0.000, 4.000], mean observation: 148.864 [0.000, 1000.000], loss: 3.649266, mae: 19.013676, mean_q: -23.368370\n",
      " 48664/50000: episode: 519, duration: 2.785s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.872 [0.000, 4.000], mean observation: 127.511 [0.000, 1000.000], loss: 2.899890, mae: 19.041559, mean_q: -23.413382\n",
      " 48711/50000: episode: 520, duration: 2.004s, episode steps: 47, steps per second: 23, episode reward: -45.000, mean reward: -0.957 [-1.000, 1.000], mean action: 2.064 [0.000, 4.000], mean observation: 143.130 [0.000, 1000.000], loss: 4.392622, mae: 19.090452, mean_q: -23.435635\n",
      " 48822/50000: episode: 521, duration: 2.487s, episode steps: 111, steps per second: 45, episode reward: -109.000, mean reward: -0.982 [-1.000, 1.000], mean action: 1.829 [0.000, 4.000], mean observation: 132.173 [0.000, 1000.000], loss: 3.010931, mae: 19.087259, mean_q: -23.467747\n",
      " 48824/50000: episode: 522, duration: 1.673s, episode steps: 2, steps per second: 1, episode reward: 0.000, mean reward: 0.000 [-1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 50.900 [0.000, 1000.000], loss: 4.886067, mae: 19.127888, mean_q: -23.517231\n",
      " 48973/50000: episode: 523, duration: 2.752s, episode steps: 149, steps per second: 54, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.926 [0.000, 4.000], mean observation: 133.101 [0.000, 1000.000], loss: 2.903346, mae: 19.120878, mean_q: -23.504923\n",
      " 49122/50000: episode: 524, duration: 2.863s, episode steps: 149, steps per second: 52, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.919 [0.000, 4.000], mean observation: 135.662 [0.000, 1000.000], loss: 2.789806, mae: 19.159966, mean_q: -23.553371\n",
      " 49231/50000: episode: 525, duration: 2.541s, episode steps: 109, steps per second: 43, episode reward: -107.000, mean reward: -0.982 [-1.000, 1.000], mean action: 2.055 [0.000, 4.000], mean observation: 143.149 [0.000, 1000.000], loss: 4.593151, mae: 19.226118, mean_q: -23.581656\n",
      " 49326/50000: episode: 526, duration: 2.416s, episode steps: 95, steps per second: 39, episode reward: -93.000, mean reward: -0.979 [-1.000, 1.000], mean action: 2.105 [0.000, 4.000], mean observation: 152.915 [0.000, 1000.000], loss: 2.964085, mae: 19.220705, mean_q: -23.602747\n",
      " 49475/50000: episode: 527, duration: 2.826s, episode steps: 149, steps per second: 53, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.960 [0.000, 4.000], mean observation: 144.613 [0.000, 1000.000], loss: 2.989633, mae: 19.249849, mean_q: -23.631943\n",
      " 49624/50000: episode: 528, duration: 2.903s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.242 [0.000, 4.000], mean observation: 128.198 [0.000, 1000.000], loss: 2.740998, mae: 19.284046, mean_q: -23.701487\n",
      " 49773/50000: episode: 529, duration: 2.926s, episode steps: 149, steps per second: 51, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 131.501 [0.000, 1000.000], loss: 3.033481, mae: 19.329588, mean_q: -23.745905\n",
      " 49922/50000: episode: 530, duration: 3.004s, episode steps: 149, steps per second: 50, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 2.134 [0.000, 4.000], mean observation: 131.569 [0.000, 1000.000], loss: 2.577321, mae: 19.363724, mean_q: -23.782959\n",
      " 49928/50000: episode: 531, duration: 1.945s, episode steps: 6, steps per second: 3, episode reward: -4.000, mean reward: -0.667 [-1.000, 1.000], mean action: 2.667 [1.000, 4.000], mean observation: 101.033 [0.000, 1000.000], loss: 7.979704, mae: 19.473562, mean_q: -23.799898\n",
      " 49934/50000: episode: 532, duration: 2.002s, episode steps: 6, steps per second: 3, episode reward: -4.000, mean reward: -0.667 [-1.000, 1.000], mean action: 2.333 [1.000, 3.000], mean observation: 101.033 [0.000, 1000.000], loss: 4.338802, mae: 19.414251, mean_q: -23.856689\n",
      " 49939/50000: episode: 533, duration: 1.912s, episode steps: 5, steps per second: 3, episode reward: -3.000, mean reward: -0.600 [-1.000, 1.000], mean action: 1.600 [1.000, 3.000], mean observation: 80.860 [0.000, 1000.000], loss: 2.160892, mae: 19.384680, mean_q: -23.790241\n",
      " 49940/50000: episode: 534, duration: 1.957s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.000 [0.000, 0.000], loss: 0.185314, mae: 19.356426, mean_q: -23.852848\n",
      " 49941/50000: episode: 535, duration: 1.923s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.000 [0.000, 0.000], loss: 0.181437, mae: 19.354404, mean_q: -23.852890\n",
      " 49942/50000: episode: 536, duration: 1.940s, episode steps: 1, steps per second: 1, episode reward: 1.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.000 [0.000, 0.000], loss: 0.179470, mae: 19.351660, mean_q: -23.853168\n",
      "done, took 1419.384 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "Exception ignored in: <bound method Env.__del__ of <__main__.FlatlandEnv object at 0x0000020E09AB2630>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\rl\\core.py\", line 686, in __del__\n",
      "    self.close()\n",
      "  File \"<ipython-input-49-4d45ef41be0e>\", line 74, in close\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\utils\\rendertools.py\", line 66, in close_window\n",
      "    self.renderer.close_window()\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\utils\\rendertools.py\", line 771, in close_window\n",
      "    self.gl.close_window()\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\utils\\graphics_pgl.py\", line 51, in close_window\n",
      "    self.window.close()\n",
      "AttributeError: 'PGLGL' object has no attribute 'window'\n",
      "Exception ignored in: <bound method Env.__del__ of <__main__.FlatlandEnv object at 0x0000020E08F04080>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\rl\\core.py\", line 686, in __del__\n",
      "    self.close()\n",
      "  File \"<ipython-input-39-0480d6fbd52a>\", line 72, in close\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\utils\\rendertools.py\", line 66, in close_window\n",
      "    self.renderer.close_window()\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\utils\\rendertools.py\", line 771, in close_window\n",
      "    self.gl.close_window()\n",
      "  File \"C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\utils\\graphics_pgl.py\", line 51, in close_window\n",
      "    self.window.close()\n",
      "AttributeError: 'PGLGL' object has no attribute 'window'\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (1, 8) -> (8, 2)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (1, 8) -> (8, 2)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (1, 8) -> (8, 2)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 4) -> (3, 8)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 4) -> (3, 8)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 4) -> (3, 8)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (2, 5) -> (8, 5)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (2, 5) -> (8, 5)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (2, 5) -> (8, 5)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 1) -> (6, 6)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 1) -> (6, 6)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 1) -> (6, 6)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 0) -> (4, 2)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (3, 5) -> (2, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 9) -> (3, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 5) -> (5, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (3, 0) -> (2, 7)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 0) -> (4, 2)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (3, 5) -> (2, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 9) -> (3, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 5) -> (5, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (3, 0) -> (2, 7)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 0) -> (4, 2)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (3, 5) -> (2, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (5, 9) -> (3, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (6, 5) -> (5, 0)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n",
      "C:\\Users\\pqman\\.conda\\envs\\flatland-rl\\lib\\site-packages\\flatland\\envs\\schedule_generators.py:286: UserWarning: reset position for agent[0]: (3, 0) -> (2, 7)\n",
      "  \"reset position for agent[{}]: {} -> {}\".format(i, agents_position[i], agents_target[i]))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x20e84f0d7f0>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -38.000, steps: 40\n",
      "Episode 2: reward: -38.000, steps: 40\n",
      "Episode 3: reward: -1.000, steps: 3\n",
      "Episode 4: reward: -1.000, steps: 3\n",
      "Episode 5: reward: -1.000, steps: 3\n",
      "Episode 6: reward: -149.000, steps: 149\n",
      "Episode 7: reward: -149.000, steps: 149\n",
      "Episode 8: reward: -149.000, steps: 149\n",
      "Episode 9: reward: -149.000, steps: 149\n",
      "Episode 10: reward: -149.000, steps: 149\n",
      "-82.4\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 2 episodes ...\n",
      "open_window - pyglet\n",
      "Episode 1: reward: -149.000, steps: 149\n",
      "open_window - pyglet\n",
      "Episode 2: reward: -149.000, steps: 149\n"
     ]
    }
   ],
   "source": [
    "_ = dqn.test(env, nb_episodes=2, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%% SAVING\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model')\n",
    "dqn.save_weights('dqn_weights.hdf5', overwrite=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}